# Many Facet Rasch Model {#MFR_model}


This chapter provides a basic overview of the Many-Facet Rasch Model (MFRM; -[] Cheng please add reference to Linacre, 1989), along with guidance for analyzing data with the MFRM using R. We use the TAM package (-[] Cheng please add reference to Robitzsch et al., 2020) for all of the analyses in this chapter. We use data from two performance assessments to demonstrate the analyses. In the first example, we demonstrate how to apply the MFRM to multi-faceted data that are stored in *wide format* (one row for each subject). Then, we demonstrate how to apply the MFRM to multi-faceted data that are stored in *long format* (multiple rows for each subject). After the analyses are complete, we present an example description of the results. The chapter concludes with a challenge exercise.

## Many-Facet Rasch Model

The Many-Facet Rasch Model (MFRM) was developed by Mike Linacre in his dissertation research with Ben Wright at the University of Chicago (-[] Cheng please add reference to Linacre, 1989). Since that time, it has been widely used across many measurement contexts.

The MFRM (-[] Cheng please add reference to Linacre, 1989) is an extension of the Rasch family of models that allows researchers to include additional variables of interest ("facets") besides items and persons. Bond and Fox (-[] SW needs to look this up) defined facets as "aspects of the measurement process that “routinely and systematically interpose themselves between the ability of the candidates and the difficulty of the test” (p. 167). Examples of interposing variables that could be modeled as facets include raters in a constructed-response assessment, participant demographic variables (e.g., gender, race/ethnicity, best language), item type, or domains in an analytic scoring rubric for constructed-response items.

A general equation for the MFRM is:

-[] Cheng, Please add Equation 6.1 here

In Equation 6.1, $\theta$ is person ability and $\tau_k$ is the rating scale category threshold, which can be modified to reflect the PC model as needed. $\Sigma_facets_$$\epsilon$ is a linear combination of facets that are specific to each modeling context. For example, facets for MFR model analysis of a performance assessment could include raters and domains. According to Equation 6.1, the probability for an observation in category *k*, rather than in category *k*- 1 for Person *n* is modeled as the difference between Person n's location, the location of the researcher-specified facets, and the difficulty associated with providing a response in category *k*.

Researchers can specify formulations of the MFR model to extend the dichotomous Rasch model (see Chapter 2) the Rating Scale (RS) model (see Chapter 4), the Partial Credit model (see Chapter 5), as well as other Rasch models (e.g., the binomial trials model and the Poisson counts model; -[] Cheng please add reference to Wright & Mok, 2004).

### Model Requirements

The MFRM is based on the same requirements of unidimensionality, local independence, and invariance that we discussed in Chapter 2 for the dichotomous Rasch model. In practice, researchers should evaluate item responses for evidence that they approximate Rasch model requirements before examining model estimates in detail. Chapter 3 included details about model-data fit analysis procedures that can also be applied to the MFRM.

# Example 1: Running the Many-Facet Rasch Model for Wide-Format Data using the TAM Package

In the next section, we provide a step-by-step demonstration of a MFRM analysis using the TAM package for R (-[] Cheng please add reference to Robitzsch et al., 2020). We encourage readers to use the example data set for this chapter that is provided in the online supplement to conduct the analysis along with us.

For this first example, we use a subset of the writing assessment data that only includes students' scores related to the style of their writing. In the second example in this chapter, we use students' scores related to four domains: Style, Organization, Conventions, and Sentence Formation.

## Getting Started

To get started with the TAM package, view the citation information, and then install and load it into your R environment using the following code:
```{r}
citation("TAM")
# install.packages("TAM")
#install.packages("TAM")
library("TAM")
```

We will also use the *WrightMap* package (Torres Irribarra & Freund, 2014):
```{r}
citation("WrightMap")
# install.packages("WrightMap")
library("WrightMap") 
```

Finally, we will use the *psych* package (Revelle, 2016):
```{r}
citation("psych")
# install.packages("psych")
library("psych") 
```


## Prepare the data for analysis

Now that we have installed and loaded the packages to our R session, we are ready to import the data. We will use the function *read.csv()* to import the comma-separated values (.csv) file that contains the data for the first example. We encourage readers to use their preferred method for importing data files into R or R Studio.

Please note that if you use *read.csv()* to import the data, you will need to specify the file path to the location at which the data file is stored on your computer or set your working directory to the folder in which you have saved the data.

First, we will import the data using *read.csv()* and store it in an object called *style*:

```{r}
style <- read.csv("style_ratings.csv")
```
The style ratings file is in *wide format*, because there is one row for each of the 372 unique students. We can see this structure by printing the first six rows of the data frame object:

```{r}
head(style)
```

Next, we will explore the data using descriptive statistics using the *summary()* function:
```{r}
summary(style)
```
From the summary of *style*, we can see there are no missing data. In addition, we can get a general sense of the scales, range, and distribution of each variable in the data set. For example, we can see that the data include student identification numbers, a language subgroup variable, and ratings from 21 raters. We can see that student identification numbers range from 3 to 1574. There are two language subgroups: Subgroup 1 (language = 1) indicates that students' best language is a language other than English, and subgroup 2 (language = 2) indicates that students' best language is English. The minimum rating from each rater was *x* = 0, and the maximum rating was *x* = 3.

Please note that the TAM package requires that the lowest observation for item responses is equal to zero. In our data, this property is already present. If the lowest category is something other than zero, the analyst will need to recode the responses as we have done in previous chapters.

## Specify the components of the MFRM

Now, we are ready to run the MFRM on the style ratings. Because the MFRM equation is researcher-specified, we need to define the components of the model. To do this, we will create an object called *facets* in which we specify the facets in the model. By default, the TAM package treats the variables that make up the columns of our item response matrix as an "item" facet. In our example, raters function as pseudo-items. Accordingly, raters make up the first facet in our analysis. Our second facet will be student language subgroups. We specify this facet and save it in a data frame object called *facets*:

```{r}
facets <- style[,"language", drop=FALSE]
```

Next we need to identify the indicator variable for the object of measurement (i.e., subject). In our example, students are the object of measurement. We will save the student identification numbers in a vector called *students*:

```{r}
students <- style$student
```

Finally, we need to specify the response matrix. We do so by extracting the raters' scores for each student to a data frame object called *ratings*:

```{r}
ratings <- style[, -c(1:2)]
```

Next, we need to specify the formula for our MFRM. For the first example will use a rating scale model specification of the MFRM. This means that we will constrain the threshold parameters to be equal across raters. The model is specified as follows:

-[]Cheng please add Equation 6.2 here

In Equation 6.2, $\theta_n$ is defined as in Equation 6.1. $\gamma_j$ is the logit-scale location for student language subgroup *j*, $\lambda_i$ is the logit-scale location for rater *i*, and $\tau_k$ is the logit-scale location at which there is an equal probability for a rating in category *k* and category *k*-1. The subgroup facet ($\gamma_j$) reflects the overall location of students in subgroup *j*, where higher locations indicate higher levels of writing proficiency, and lower locations indicate lower levels of writing proficiency. The rater facet ($\lambda_i$) reflects the severity level of individual rater *i*, where higher locations mean that the rater is more severe, and requires higher levels of writing proficiency before giving high ratings to student performances. Lower rater locations indicate relatively lenient raters, who more readily give high ratings to student performances.

We specify the MFRM from Equation 6.2 in an object for use with TAM as follows. First, we specify a name for the model object(*style_RS_MFRM*), which is defined using the tilde symbol (~), followed by the facet names. As a reminder, the model must include a facet named *item*; in our example, the item facet is made up of raters. We also include the student language subgroup (*language*) as a facet. Finally, we specify *step* to indicate the RS model. The components of the model are separated by addition signs (+) because the facets are additive. We will specify interactions in a MFRM later in this chapter.

```{r}
style_RS_MFRM <- ~ item + language + step
```

## Run the RS-MFRM

Now we can run our RS-MFRM. We do so using the *tam.mml.mfr()* function, in which we specify the response matrix (resp=), our specified facets (facets=), the model equation (formulaA=), and the identification numbers for the object of measurement (pid=).

After we run the model, we will request a summary of the model results using the *summary()* function.
-[] omit the model run output
```{r results='hide'}
RS_MFR_model <- tam.mml.mfr(resp = ratings, facets = facets, formulaA = style_RS_MFRM, pid = students)
```

The MFRM function produces lengthy output. Included among the output are several details that may be important for some researchers, including details about each iteration, global model-fit indicators (e.g., deviance,  log-likelihood, AIC, BIC) and an estimate of person separation reliability (EAP Reliability). 

We will focus our interpretation on the location estimates for the student, subgroup, rater, and threshold parameters.

## Examine facet results:

Next, we will save the parameter estimates from the RS-MFRM in a data frame object called *facet.estimates*. This object includes the location estimates and standard errors for raters, student subgroups, and thresholds. Locations estimates are labeled *xsi* and standard errors are labeled *se.xsi*.
```{r}
facet.estimates <- RS_MFR_model$xsi.facets
```

For easier reference, we will now create objects in which we store the location estimates and standard errors for raters, subgroups, and thresholds separately. We do this by applying the *subset()* function to the *facet.estimates* object:
```{r}
rater.estimates <- subset(facet.estimates, facet.estimates$facet == "item")
subgroup.estimates <- subset(facet.estimates, facet.estimates$facet == "language")
threshold.estimates <- subset(facet.estimates, facet.estimates$facet == "step")
```

### Rater facet results
We will center the rater parameter estimates for ease of interpretation, as we have done previously with item estimates. Then, we will request a summary of the centered rater locations.
```{r}
uncentered.rater.locations_RSMFR <- rater.estimates$xsi
centered.rater.locations_RSMFR <- scale(uncentered.rater.locations_RSMFR, scale = FALSE)
summary(centered.rater.locations_RSMFR)
```

From the summary of rater locations, we can see that rater severity estimates range from -0.86 for the most lenient rater to 1.11 for the most severe rater.

### Student subgroup facet results
We do not need to adjust the location of the subgroups to account for centering the rater facet because the facet locations are already centered at zero logits. We will examine the language subgroup estimates using the *summary()* function. Because there are only two subgroups in our analysis, we can also print the locations to our console to inspect them. 
```{r}
summary(subgroup.estimates$xsi)
subgroup.estimates$xsi
```

From these results, we can see that the two student subgroup locations are quite close. As a group, students whose best language is not English (subgroup 1) had a slightly lower location on the logit scale ($\gamma_1$ = -0.04 logits) compared to students whose best language was not English (subgroup 2; $\gamma_2$ = 0.04 logits). Although there was a difference in subgroup locations, the difference was very small (about 0.09 logits), and therefore likely does not reflect a substantively meaningful difference in writing achievement between these two groups.

### Threshold estimates
Next we will examine the threshold estimates. Because we used a RS formulation of the MFRM, there is one set of threshold estimates for our model that applies across raters. We will print the threshold estimates to the console to view them.
```{r}
threshold.estimates$xsi
```

### Student estimates
Next we will examine the student location estimates from the RS-MFRM. To estimate student locations, we need to apply the *tam.wle()* function to our model object (*RS_MFR_model*) and store the results in an object called *student.ach*. We will then store the student identification numbers, location estimates, and standard errors in a new object called *student.locations_RSMFR*.
```{r results='hide'}
student.ach <- tam.wle(RS_MFR_model)
student.locations_RSMFR <- cbind.data.frame(student.ach$pid, student.ach$theta, student.ach$error)
names(student.locations_RSMFR) <- c("id", "theta", "se")
```

To interpret the student locations in the same frame of reference as our centered rater locations, we need to subtract the original (uncentered) rater mean location from the student locations:
```{r}
student.locations_RSMFR$theta_adjusted <- student.locations_RSMFR$theta - mean(uncentered.rater.locations_RSMFR)
summary(student.locations_RSMFR$theta_adjusted)
```

From the summary of the adjusted student achievement locations, we can see that student achievement ranges from -7.91 logits for the student with the lowest achievement estimate to 7.61 for the student with the highest achievement estimate. On average, the students were located slightly higher (M = 0.46) than the average rater location (M = 0.00).

### Plot a Wright Map

Next, we will plot a Wright Map to display the locations of the parameter estimates for our RS-MFR model. To do this, we need to manipulate the location estimate objects into the format required for the WrightMap package. 

First, we need to store the rater location estimates as a matrix that shows rater-specific threshold locations. We accomplish this task using a loop in which we add each rater's location to the three threshold values and store the results in a matrix called *rater_thresholds*.
```{r}
rater_thresholds <- matrix(data = NA, nrow = length(centered.rater.locations_RSMFR), ncol = nrow(threshold.estimates))

for(rater in 1:length(centered.rater.locations_RSMFR)){
  for(tau in 1:nrow(threshold.estimates)){
  rater_thresholds[rater,tau] <- (centered.rater.locations_RSMFR[rater] + threshold.estimates$xsi[tau])
  }                                
}
```

Next, we need to store the adjusted student locations and the subgroup facet locations in a single data frame. Doing so will allow us to plot the locations for students and subgroups in the same Wright Map. We store these values in a data frame object called *persons*.
```{r}
persons <- cbind.data.frame(student.locations_RSMFR$theta_adjusted, subgroup.estimates$xsi)
```

Finally, we can plot the Wright map using the *wrightMap()* function. We specify several graphical parameters to modify the appearance of the plot.
```{r}
wrightMap(thetas = persons,
          axis.persons = "Students",
          dim.names = c("Students", "Subgroups"), 
          thresholds = rater_thresholds,
          show.thr.lab	= TRUE,
          label.items.rows= 2,
          label.items = rater.estimates$parameter,
          axis.items = "Raters",
          main.title = "Rating Scale Many-Facet Rasch Model Wright Map:\n Style Ratings",
          cex.main = .6)
```

In this *Wright Map* display, the results from the RS-MFRM analysis of the style ratings are summarized graphically. The figure is organized as follows:

* Units on the logit scale are shown on the far-right axis of the plot (labeled *Logits*). 

* The left-most panel of the plot shows a histogram of student locations on the logit scale that represents the latent variable. 

* The second panel from the left shows the distribution of subgroups on the logit scale. There are only two subgroups in our analysis.

The large central panel of the plot shows the rating scale category threshold estimates specific to each rater on the logit scale that represents the latent variable. Light grey diamond shapes show the logit scale location of the threshold estimates for each rater, as labeled on the x-axis. Thresholds are labeled using tau symbols followed by an integer that shows the threshold number. In our example, $\tau$1 is the threshold between rating scale categories *x* = 0 and *x* = 1, $\tau$2 is the threshold between rating scale categories *x* = 1 and *x* = 2, and $\tau$3 is the threshold between rating scale categories *x* = 2 and *x* = 3. Because we used a RS model formulation, the distance between adjacent thresholds is the same for all of the raters in the analysis.

Even though it is not appropriate to fully interpret item and person locations on the logit scale until there is evidence of acceptable model-data fit, we recommend examining the Wright Map during the preliminary stages of a MFRM analysis to get a general sense of the model results and to identify any potential scoring or data entry errors.

The Wright Map suggests that, on average, the students are located higher on the logit scale compared to the average rater threshold locations. In addition, there appears to be a relatively wide spread of student and rater locations on the logit scale, such that the style writing assessment appears to be a useful tool for identifying differences in students' writing achievement related to style as well as differences in rater severity. The subgroup locations are close together, suggesting that there is not much difference in the logit-scale locations between students in either language subgroup.

## Evaluate model-data fit

Because the MFR model analyses result in notably different output structures from the other models in this book, we demonstrate a procedure for evaluating model-data fit specific to the MFRM with the TAM package. These procedures generally follow the methods that we presented in Chapter 3. We encourage readers to refer to Chapter 3 for a detailed overview of model-data fit analysis procedures in the context of Rasch measurement theory.

### Evaluate unidimensionality

First, we will evaluate the MFRM requirement for unidimensionality using the same procedure that we demonstrated in Chapter 3. We will evaluate this requirement by examining the residuals for evidence of potential secondary latent variables in the rating data.

First, we will extract the model residuals using the *IRT.residuals()* function from TAM. We will save the results in an object called *resids*.
```{r}
resids <- IRT.residuals(RS_MFR_model)
```

With the MFRM in TAM, residuals are presented separately for each level of the explanatory facets.We need to do some manipulation to construct a typical residual matrix.
```{r}
# Extract the raw residuals from the residuals object:
r <- as.data.frame(resids$residuals)

# Save the residuals in a matrix:
resid.matrix <- matrix(data = NA, nrow = nrow(style), ncol = ncol(ratings))

ngroups <- nrow(subgroup.estimates)

for(rater.number in 1:ncol(ratings)){
  group.raters <- NULL
  
  for(group in 1:ngroups){
    group.raters[group] <- paste("rater_", rater.number, "-", "language", group, sep = "")
  }
  
  rater <- subset(r, select = group.raters)
  
  resid.matrix[, rater.number] <- rowSums(rater, na.rm = TRUE)
}
```

The resulting residual matrix (*resid.matrix*) contains unstandardized residuals for each rater in combination with each student. We can request a summary of the residuals using the *summary()* function.
```{r}
summary(resid.matrix)
```

Next, we will calculate standardized residuals and save them in a matrix.
```{r}
# Extract standardized residuals from the resids object:
s <- as.data.frame(resids$stand_residuals)

# Save the standardized residuals in a matrix:
std.resid.matrix <- matrix(data = NA, nrow = nrow(style), ncol = ncol(ratings))

ngroups <- nrow(subgroup.estimates)

for(rater.number in 1:ncol(ratings)){
  group.raters <- NULL
  
  for(group in 1:ngroups){
    group.raters[group] <- paste("rater_", rater.number, "-", "language", group, sep = "")
  }
  
  rater <- subset(s, select = group.raters)
  
  std.resid.matrix[, rater.number] <- rowSums(rater, na.rm = TRUE)
}
```

The resulting standardized residual matrix (*std.resid.matrix*) contains standardized residuals for each rater in combination with each student. We can request a summary of the standardized residuals using the *summary()* function.
```{r}
summary(std.resid.matrix)
```

Next, we will calculate the variance in observations due to Rasch-model-estimated locations:
```{r}
# Variance of the observations: VO
observations.vector <- as.vector(as.matrix(ratings))
VO <- var(observations.vector)
VO

# Variance of the residuals: VR
residuals.vector <- as.vector(resid.matrix)
VR <- var(residuals.vector)
VR

# Raw variance explained by Rasch measures: (VO - VR)/VO
(VO - VR)/VO

# Express the result as a percent:
((VO - VR)/VO) * 100
```

Our analysis indicates that approximately 74.18% of the variance in ratings can be explained by the MFRM estimates of student, subgroup, and rater locations on the logit scale that represents the latent variable. 

### Principal Components Analysis of Standardized Residual Correlations

Next, we will evaluate the MFRM requirement for unidimensionality using a principal components analysis (PCA) of standardized residual correlations.

```{r}
pca <- pca(std.resid.matrix, nfactors = ncol(ratings), rotate = "none")

contrasts <- c(pca$values[1], pca$values[2], pca$values[3], pca$values[4], pca$values[5])

plot(contrasts, ylab = "Eigenvalues for Contrasts", xlab = "Contrast Number", main = "Contrasts from PCA of Standardized Residual Correlations", ylim = c(0, 2))
```

In this example, all of the contrasts have eigenvalues that are smaller than Linacre's (2016) critical value of 2.00. This result suggests that the correlations among the model residuals primarily reflect randomness (i.e., noise)--thus providing evidence that the responses adequately adhere to the Rasch model requirement of unidimensionality.

### Summaries of Residuals: Infit & Outfit Statistics

Next, we will evaluate model-data fit for individual elements of our facets (students, subgroups, and raters) using numeric summaries of the residuals associated with each element, as we have done in previous chapters.

***Student fit***

First, we will examine student fit using numeric infit and outfit statistics. We can request these statistics for each student using the *tam.personfit()* function. We will store the student fit results in an object called *student.fit*, and then request a summary of the results.
```{r}
student.fit <- tam.personfit(RS_MFR_model)
summary(student.fit)
```

On average, the MSE outfit and infit statistics are slightly lower than the expected value of 1 (Mean Outfit = 0.89, Mean Infit = 0.90). The average values of the standardized fit statistics are also slightly lower than their expected value of 0 (Mean Std. Outfit = -0.33, Mean Std. Infit = -0.31). For both the standardized and unstandardized fit statistics, there is notable variability across the student sample. This result suggests that model-data fit varies for individual students.

***Subgroup and rater fit***

We can also examine model-data fit related to the subgroup and rater facets. In the TAM package, fit analysis for facets besides the object of measurement uses combinations of elements within facets. In our example, fit statistics are calculated for rater*subgroup combinations.
```{r}
rater.subgroup.fit <- msq.itemfit(RS_MFR_model)
summary(rater.subgroup.fit)
```

As needed, researchers can also examine model-data fit statistics specific to subgroups of examinees. This can be accomplished by calculating summary statistics of person fit statistics within examinee subgroups. The code below merges the person fit results with student subgroup identification numbers, and then calculates summary statistics and produces boxplots of the fit statistics by group.
```{r}
fit_with_subgroups <- cbind.data.frame(style$language, student.fit)

fit_group_1 <- subset(fit_with_subgroups, fit_with_subgroups$`style$language` == 1)
summary(fit_group_1)

fit_group_2 <- subset(fit_with_subgroups, fit_with_subgroups$`style$language` == 2)
summary(fit_group_2)

# Boxplots for MSE fit statistics:
boxplot(fit_group_1$outfitPerson, fit_group_2$outfitPerson,
        fit_group_1$infitPerson, fit_group_2$infitPerson,
        names = c("Group 1 \nOutfit MSE", "Group 2 \nOutfit MSE", 
                  "Group 1 \nInfit MSE", "Group 2 \nInfit MSE"),
        col = c("grey", "white", "grey", "white"),
        main = "MSE Fit Statistics for English-not-Best-Language (Group 1) \nand English-Best-Language (Group 2) Students",
        cex.main = .8,
        ylab = "MSE Fit Statistic", xlab = "Student Subgroup")

# Boxplots for standardized fit statistics:
boxplot(fit_group_1$outfitPerson_t, fit_group_2$outfitPerson_t,
        fit_group_1$infitPerson_t, fit_group_2$infitPerson_t,
        names = c("Group 1 \nStd. Outfit", "Group 2 \nStd. Outfit", 
                  "Group 1 \nStd. Infit", "Group 2 \nStd. Infit"),
        col = c("grey", "white", "grey", "white"),
        main = "Standardized Fit Statistics for English-not-Best-Language (Group 1) \nand English-Best-Language (Group 2) Students",
        cex.main = .8,
        ylab = "MSE Fit Statistic", xlab = "Student Subgroup")
```

Finally, it may be useful to examine fit statistics as they apply to individual raters. This can be accomplished by extracting rater-specific fit statistics within each subgroup. The code below calculates rater fit statistics within each subgroup.
```{r}
rater.subgroup.fit$itemfit

ngroups <- nrow(subgroup.estimates)

rater.fit <- matrix(data = NA, nrow = ncol(ratings), ncol = (ngroups * 4) + 1 )

for(rater.number in 1:ncol(ratings)){
  
  # calculate rater-specific fit statistics:
  rater.outfit <- rater.subgroup.fit$itemfit$Outfit[((rater.number*ngroups) - (ngroups - 1)) : (rater.number*ngroups)]
  
    rater.infit <- rater.subgroup.fit$itemfit$Infit[((rater.number*ngroups) - (ngroups - 1)) : (rater.number*ngroups)]
    
   rater.std.outfit <- rater.subgroup.fit$itemfit$Outfit_t[((rater.number*ngroups) - (ngroups - 1)) : (rater.number*ngroups)]
    
  rater.std.infit <- rater.subgroup.fit$itemfit$Infit_t[((rater.number*ngroups) - (ngroups - 1)) : (rater.number*ngroups)]
       
  # add the fit statistics to the matrix:  
  rater.fit[rater.number, ] <-  c(rater.number, rater.outfit, rater.infit,
                                  rater.std.outfit, rater.std.infit)
}


# Convert the rater fit results to a dataframe object and add meaningful column names:  

rater.fit_results <- as.data.frame(rater.fit)

infit_mse_labels <- NULL
for(group in 1:ngroups){
  infit_mse_labels[group] <- paste("Infit_MSE_Group", group, sep = "")
}

outfit_mse_labels <- NULL
for(group in 1:ngroups){
  outfit_mse_labels[group] <- paste("Outfit_MSE_Group", group, sep = "")
  }

std_infit_mse_labels <- NULL
for(group in 1:ngroups){
  std_infit_mse_labels[group] <- paste("Std.Infit_MSE_Group", group, sep = "")
}

std_outfit_mse_labels <- NULL
for(group in 1:ngroups){
  std_outfit_mse_labels[group] <- paste("Std.Outfit_MSE_Group", group, sep = "")
  }

names(rater.fit_results) <- c("Rater", outfit_mse_labels, infit_mse_labels,
                              std_outfit_mse_labels, std_infit_mse_labels)
```

Now that we have created a data frame with the rater-specific fit statistics, we can summarize the results.
```{r}
summary(rater.fit_results)
```

***Graphical displays of residuals***

Continuing our fit analysis, we will construct plots of standardized residuals associated with individual raters. These plots can demonstrate patterns in unexpected and expected responses that can be useful for understanding responses and interpreting results specific to individual raters. In other applications of the MFRM, researchers can construct similar plots for other facets.

Earlier in this chapter, we stored the standardized residuals in an object called *std.resid.matrix*. We will use this object to create plots for individual raters via a for-loop. For brevity, we have only included plots for the first three raters in this book. The specific raters to be plotted can be controlled by changing the items included in the *raters.to.plot* object.
```{r}
# Before constructing the plots, find the maximum and minimum values of the standardized residuals to set limits for the axes:
max.resid <- ceiling(max(std.resid.matrix))
min.resid <- ceiling(min(std.resid.matrix))

# The code below will produce plots of standardized residuals for selected raters as listed in raters.to.plot:
raters.to.plot <- c(1:3)

for(rater.number in raters.to.plot){
  plot(std.resid.matrix[, rater.number], ylim = c(min.resid, max.resid),
       main = paste("Standardized Residuals for Rater ", rater.number, sep = ""),
       ylab = "Standardized Residual", xlab = "Person Index")
  abline(h = 0, col = "blue")
  abline(h=2, lty = 2, col = "red")
  abline(h=-2, lty = 2, col = "red")
  
  legend("topright", c("Std. Residual", "Observed = Expected", "+/- 2 SD"), pch = c(1, NA, NA), 
         lty = c(NA, 1, 2),
         col = c("black", "blue", "red"), cex = .8)
}
```

A separate plot is produced for each rater In each plot, the y-axis shows values of the standardized residuals, and the x-axis shows the students, ordered by their relative position in the data set. Open-circle plotting symbols show the standardized residual associated with each student's rating from the rater of interest.

Horizontal lines are used to assist in the interpretation of the values of the standardized residuals. First, a solid line is plotted at a value of 0; standardized residuals equal to zero indicate that the observed response was equal to the model-expected response given student and rater locations. Standardized residuals that are greater than zero indicate unexpectedly high ratings, and standardized residuals that are less than zero indicate unexpectedly low ratings. Dashed lines are plotted at values of +2 and -2 to indicate standardized residuals that are two standard deviations above or below model expectations, respectively. Researchers often interpret standardized residuals that exceed +/- 2 as indicating statistically significant unexpected responses. 

***Expected and observed response functions***
As a final step in our fit analysis, we will construct plots of expected and observed response functions. By default, the TAM package combines the item facet (in this case, raters), with levels of the other facets (in this case, language subgroups) when constructing expected and observed response function plots. 

For brevity, we only plot the expected and observed response functions for three selected rater*item combinations. Readers can adjust the "items=" specification to construct plots for elements of interest for their analyses.
```{r}
plot(RS_MFR_model, type = "expected", items = c(1:3))
```

## Summarize the results in tables
As a final step, we will create tables that summarize the calibrations of the students, subgroups, raters, and rating scale category thresholds.

Table 1 is an overall model summary table that provides an overview of the logit scale locations, standard errors and fit statistics for all of the facets in the analysis. This table provides a quick overview of the location estimates and numeric model-data fit statistics for the facets in a MFRM.

Because of the estimation procedure for the MFRM in TAM, fit statistics are combined for the item facet and other facets. As a result, the fit statistics in this table will be the same for the rater facet and the subgroup facets.

### Model summary table:
```{r}
RS_MFRM_summary.table.statistics <- c("Logit Scale Location Mean",
                              "Logit Scale Location SD",
                              "Standard Error Mean",
                              "Standard Error SD",
                              "Outfit MSE Mean",
                              "Outfit MSE SD",
                              "Infit MSE Mean",
                              "Infit MSE SD",
                              "Std. Outfit Mean",
                              "Std. Outfit SD",
                              "Std. Infit Mean",
                              "Std. Infit SD")
                              
RS_MFRM_student.summary.results <- rbind(mean(student.locations_RSMFR$theta_adjusted),
                              sd(student.locations_RSMFR$theta_adjusted),
                              mean(student.locations_RSMFR$se),
                              sd(student.locations_RSMFR$se),
                              mean(student.fit$outfitPerson),
                              sd(student.fit$outfitPerson),
                              mean(student.fit$infitPerson),
                              sd(student.fit$infitPerson),
                              mean(student.fit$outfitPerson_t),
                              sd(student.fit$outfitPerson_t),
                              mean(student.fit$infitPerson_t),
                              sd(student.fit$infitPerson_t))


RS_MFRM_subgroup.summary.results <- rbind(mean(subgroup.estimates$xsi),
                              sd(subgroup.estimates$xsi),
                              mean(subgroup.estimates$se.xsi),
                              sd(subgroup.estimates$se.xsi),
                              mean(rater.subgroup.fit$itemfit$Outfit),
                              sd(rater.subgroup.fit$itemfit$Outfit),
                              mean(rater.subgroup.fit$itemfit$Infit),
                              sd(rater.subgroup.fit$itemfit$Infit),
                              mean(rater.subgroup.fit$itemfit$Outfit_t),
                              sd(rater.subgroup.fit$itemfit$Outfit_t),
                              mean(rater.subgroup.fit$itemfit$Infit_t),
                              sd(rater.subgroup.fit$itemfit$Infit_t))

RS_MFRM_rater.summary.results <- rbind(mean(rater.estimates$xsi),
                              sd(rater.estimates$xsi),
                              mean(rater.estimates$se.xsi),
                              sd(rater.estimates$se.xsi),
                              mean(rater.subgroup.fit$itemfit$Outfit),
                              sd(rater.subgroup.fit$itemfit$Outfit),
                              mean(rater.subgroup.fit$itemfit$Infit),
                              sd(rater.subgroup.fit$itemfit$Infit),
                              mean(rater.subgroup.fit$itemfit$Outfit_t),
                              sd(rater.subgroup.fit$itemfit$Outfit_t),
                              mean(rater.subgroup.fit$itemfit$Infit_t),
                              sd(rater.subgroup.fit$itemfit$Infit_t))


# Round the values for presentation in a table:
RS_MFRM_student.summary.results_rounded <- round(RS_MFRM_student.summary.results, digits = 2)

RS_MFRM_subgroup.summary.results_rounded <- round(RS_MFRM_subgroup.summary.results, digits = 2)

RS_MFRM_rater.summary.results_rounded <- round(RS_MFRM_rater.summary.results, digits = 2)


RS_MFRM_Table1 <- cbind.data.frame(RS_MFRM_summary.table.statistics,
                           RS_MFRM_student.summary.results_rounded,
                           RS_MFRM_subgroup.summary.results_rounded,
                           RS_MFRM_rater.summary.results_rounded)
                           

# add descriptive column labels:
names(RS_MFRM_Table1) <- c("Statistic", "Students", "Subgroups", "Raters")  

# Print the table to the console
RS_MFRM_Table1
```

### Rater calibration table:
Table 2 is a table that summarizes the overall calibrations of individual students. For data sets with manageable sample sizes such as the Liking for Science data example in this chapter, we recommend reporting details about each item in a table similar to this one.

```{r}
# Calculate the average rating for each rater:
Avg_Rating <- apply(ratings, 2, mean)

# Combine rater calibration results in a table:

RS_MFRM_Table2 <- cbind.data.frame(c(1:ncol(ratings)), 
                           Avg_Rating,
                           centered.rater.locations_RSMFR,
                           rater_thresholds,
                           rater.fit_results[, -1])

names(RS_MFRM_Table2) <- c("Rater ID", "Average Rating", "Rater Location","Threshold 1", "Threshold 2", "Threshold 3", names(rater.fit_results[, -1]))                           

# Sort Table 2 by rater severity:
RS_MFRM_Table2 <- RS_MFRM_Table2[order(-RS_MFRM_Table2$`Rater Location`),]

# Round the numeric values (all columns except the first one) to 2 digits:
RS_MFRM_Table2[, -1] <- round(RS_MFRM_Table2[,-1], digits = 2)

# Print the table to the console
RS_MFRM_Table2
```

### Student calibration table:
Finally, Table 3 provides a summary of the student calibrations. When there is a relatively large person sample size, it may be more useful to present the results as they relate to individual persons or subsets of the person sample as they are relevant to the purpose of the analysis.

```{r}
# Calculate average ratings for students:
Person_Avg_Rating <- apply(ratings, 1, mean)

# Combine person calibration results in a table:
RS_MFRM_Table3 <- cbind.data.frame(rownames(student.locations_RSMFR),
                          Person_Avg_Rating,
                          student.locations_RSMFR$theta_adjusted,
                          student.locations_RSMFR$se,
                          student.fit$outfitPerson,
                          student.fit$outfitPerson_t,
                          student.fit$infitPerson,
                          student.fit$infitPerson_t)
                          
names(RS_MFRM_Table3) <- c("Student ID", "Average Rating", "Student Location","Student SE","Outfit MSE","Std. Outfit", "Infit MSE","Std. Infit")

# Round the numeric values (all columns except the first one) to 2 digits:
RS_MFRM_Table3[, -1] <- round(RS_MFRM_Table3[,-1], digits = 2)

# Print the first six rows of the table to the console
head(RS_MFRM_Table3)
```




# Example 2:  Running the Many-Facet Rasch Model for Long-Format Data using the TAM Package

In the next section, we provide a step-by-step demonstration of a MFRM analysis using the TAM package for R (-[] Cheng please add reference to Robitzsch et al., 2020) for data that are stored in long format. We encourage readers to use the example data set for this chapter that is provided in the online supplement to conduct the analysis along with us.

For this example, we use a subset of the writing assessment data that includes students' scores related to four domains: Style, Organization, Conventions, and Sentence Formation.

Compared to the first example in this chapter, our description of the second example is relatively less detailed. In cases where there are important differences between the two examples, we describe them. In other cases, we encourage readers to refer to the first example for details.

## Getting Started

Before proceeding with the analysis, readers should ensure that they have installed and loaded all three of the packages that were described earlier in this chapter: *TAM*, *WrightMap*, and *psych*:
```{r}
citation("TAM")
# install.packages("TAM")
#install.packages("TAM")
library("TAM")

citation("WrightMap")
# install.packages("WrightMap")
library("WrightMap")

citation("psych")
# install.packages("psych")
library("psych")
```

## Prepare the data for analysis
Next, we will import the data for our analysis. The data for this example are stored in the file named *writing.csv*. We will save these data in an object called *writing*.
```{r}
writing <- read.csv("writing.csv")
```

Note that the writing data are in *long* format: This means that there are multiple rows for each element within the object of measurement. In the case of our example data, this means that there are multiple rows for each student. Each row includes one rater's ratings of one student on all four of the domains in the assessment: Style, Organization, Conventions, and Sentence Formation. We can see this structure by printing the first six rows of the data frame object:
```{r}
head(writing)
```

Next, we will explore the data using descriptive statistics using the *summary()* function:

```{r}
summary(writing)
```

From the summary of *writing*, we can see there are no missing data. In addition, we can get a general sense of the scales, range, and distribution of each variable in the data set.

We can see that student identification numbers range from 3 to 1574. We can identify the number of unique students in the data using the following code:
```{r}
length(unique(writing$student))
```
There are 372 unique student identification numbers in our data. Returning to the summary of the writing data, we can see that the minimum rating on each domain was *x* = 1, and the maximum rating was *x* = 4. 

Next, let's prepare the data for analysis with TAM by shifting the scale so that the lowest category equals zero. We accomplish this by subtracting 1 from the ratings.
```{r}
writing[, -c(1:3)] <- writing[, -c(1:3)] - 1
```

## RS-MFRM Analysis with Long-Format Data

First, we will analyze the writing data using a RS-MFRM. The facets in this model will include raters and domains:

-[] Cheng, please add Equation 6.3 here.

In Equation 6.3, $\theta_n$ and $\tau_k$ are defined as in Equation 6.1 and Equation 6.2. $\delta_m$ is the logit-scale location for domain *j*. Lower domain locations indicate relatively easy domains, and higher domain locations indicate relatively difficult domains.

### Specify the MFRM:

We specify the MFRM from Equation 6.3 in an object for use with TAM as follows. First, we specify a name for the model object(*style_RS_MFRM*), which is defined using the tilde symbol (~), followed by the facet names. As a reminder, the model must include a facet named *item*; in our example, the item facet is made up of raters. We also include the student language subgroup (*language*) as a facet. Finally, we specify *step* to indicate the RS model. The components of the model are separated by addition signs (+) because the facets are additive. We will specify interactions in a MFRM later in this chapter.

With long format data, we need to ensure that the person identification numbers (in this case, student ids) are sorted from low to high before we can run the analysis.

```{r}
writing <-writing[order(writing$student),]
```

Next, we specify the components of the RS-MFRM. In this long-format data analysis, the TAM package will treat domains as "items" because they make up the columns of the response matrix. Therefore, our facets object includes raters.
```{r}
# identify the facets besides items :
writing.facets <- writing[, c("rater"), drop=FALSE] 

# identify the object of measurement:
writing.pid <- writing$student 

# identify the response matrix:
writing.resp <- writing[,-c(1:3)]

# specify the RS-MFR model:
RS.writing.formula <- ~ item + rater + step 

# Run the RS-MFR model:

RS_MFRM_writing.model <- tam.mml.mfr(resp=writing.resp,facets=writing.facets,
                             formulaA=RS.writing.formula, pid=writing.pid)

# Request a summary of the model results:
summary(writing.model)

# Save the facet estimates:
facet.estimates <- writing.model$xsi.facets # all facets together

domain.estimates <- subset(facet.estimates, facet.estimates$facet == "item")

rater.estimates <- subset(facet.estimates, facet.estimates$facet == "rater")

threshold.estimates <- subset(facet.estimates, facet.estimates$facet == "step")
```


