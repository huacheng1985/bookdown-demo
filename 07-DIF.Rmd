# Basics of Differential Item Functioning {#DIF}

This chapter provides a basic overview of differential item functioning (DIF) along with methods for detecting DIF with Rasch models using R. We will use the eRm package (Mair et al., 2020) and the TAM package (Robitzch et al., 2020) for the analyses in this chapter.

## What is Differential Item Functioning (DIF)?

DIF occurs when examinees who are members of different groups (e.g., demographic subgroups) who have the *same location* on the latent variable have *different probabilities for a response* in a given category (e.g., a correct response or a rating of "agree") on an item. In the context of Rasch measurement theory (Rasch, 1960), DIF means that an item has a different location (i.e., a different level of difficulty) across subgroups by more than the standard error. Rasch analyses of DIF typically focus on detecting *uniform DIF*, which occurs when the distance between item response functions (IRFs) is constant between subgroups.

Figure 7.1 illustrates uniform DIF between two groups of examinees for a dichotomous item (x = 0, 1). The x-axis shows examinee locations on the latent variable, and the y-axis shows the probability for a correct or positive response (P(x=1)). The dashed line shows the IRF for Group 1, and the dashed line shows IRF for Group 2. This item shows DIF because examinees who have the same location on the latent variable have different probabilities for a correct or positive response.

Figure 7.1
```{r}
IRF <- function(b_group1, b_group2){
  colors <- c("purple", "seagreen3")
  theta <- seq(-3, 3, .1)
  P1 <- 1 / (1 + exp(-(theta - b_group1)))
  P2 <- 1 / (1 + exp(-(theta - b_group2)))
  plot(theta, P1, type="l", xlim=c(-3,3), ylim=c(0,1),
       xlab="Theta", ylab="P(X=1)", col = colors[1], lty = 1,
       main = "Item i")
  lines(theta, P2, col = colors[2], lty = 2)
  legend("bottomright", c("Group 1", "Group 2"), lty = c(1, 2),
         col = c(colors[1], colors[2]))
}

IRF(b_group1 = 1, b_group2 = -1)
```

The interpretation and use of DIF indices varies along with the individual purpose and consequences of each assessment. As a result, it is critical that analysts consider the unique context in which they are evaluating DIF when they interpret the results from DIF analyses. In addition, it is often useful to consider DIF from multiple perspectives, including different statistical techniques for identifying DIF. 

It is also important to note that when DIF occurs, it does not always imply a threat to fairness from a psychometric perspective (AERA et al., 2014). Instead, evidence of DIF is a *starting place* for additional research, such as qualitative analyses of item content or the assessment context, to evaluate whether DIF may reflect a threat to fairness. 

In this chapter, we demonstrate techniques that analysts can use to identify DIF within the Rasch measurement theory framework. Our presentation is not exhaustive and there are many other methods that can be used to supplement those that we demonstrate here. We encourage interested readers to consult the resources listed throughout the chapter, as well as the resource list at the end of this chapter, to learn more about best practices for identifying DIF within the context of Rasch measurement theory, as well as in other measurement frameworks.

## Methods for Identifying DIF with Rasch models

From the perspective of Rasch measurement theory, DIF poses a threat to measurement quality because it implies a lack of invariance. When DIF occurs, the requirements for invariant measurement are not met (see Chapter 1). Accordingly, DIF can be interpreted as not only a potential threat to fairness, but also as a violation of Rasch model requirements (Hagquist & Andrich, 2017).

Researchers have proposed several methods for identifying DIF within the framework of Rasch measurement theory. Methods that are routinely used in practical applications of Rasch measurement theory can be broadly classified into three approaches: (1) Comparison of group-specific item estimates; (2) Interaction analyses between item locations and examinee subgroup indicators; and (3) Methods based on analysis of variance with residuals. There are numerous other approaches to detecting DIF that are beyond the scope of this volume. We encourage readers who are interested in learning more about DIF to consult the resources that we have listed at the end of this chapter.

***Comparing Group-Specific Item Estimates***

A popular method for identifying DIF within the framework of Rasch measurement theory is to calculate group-specific estimates of item difficulty, and compare those estimates to see if they are meaningfully different. In order to meaningfully compare item difficulty estimates between two groups, it is necessary to adjust (i.e., equate) the item difficulty measures for one group so that they are on the same scale as the item difficulty measures for the second group. The equated differences in item difficulty estimates  reflect the distance between group-specific IRFs (Raju, 1988).

Some software programs, including the eRm package (Mair et al., 2020), perform this adjustment procedure automatically as part of their DIF functions. In other cases, the analyst needs to perform the adjustment procedure manually before item locations can be compared. For a discussion of the adjustment procedure, please see Luppescu (1995).

After group-specific item difficulty estimates have been calculated and adjusted for the purpose of comparison, it is necessary to consider the degree to which differences in item difficulty estimates are large enough to indicate substantial DIF. Researchers who use Rasch models to evaluate DIF evaluate these differences using several approaches. For example, many researchers calculate the difference between item difficulty estimates as:

-[] Add Equation 7.1 here

In Equation 7.1, $/delta_i1$ is the group-specific item difficulty estimate for Group 1, and $/delta_i2$ is the group-specific item difficulty estimate for Group 2. Researchers sometimes interpret the difference in item difficulty in logits as an effect size, where larger differences indicate more substantial DIF. Wright and Douglas (1975) recommended that researchers use a critical value of 0.5 logits (i.e., the "half-logit rule") to identify DIF that warrants further investigation. This recommendation is based on the typical range of item difficulty for achievement tests (around -2.5 logits to around 2.5 logits), because a difference of 0.5 logits reflects 10% of the item difficulty scale. Differences of this magnitude may impact the accuracy of the measurement procedure between subgroups. Other researchers have proposed smaller critical values, such as Engelhard and Myford (2003), who recommended a difference of 0.3 logits to identify meaningful DIF. 

In the Winsteps software manual, Linacre (2020) proposed guidelines for interpreting differences in item difficulty that reflect the *Delta Index* from ETS (Dorans & Holland, 1993). He noted that differences less than 0.43 logits can be interpreted as negligible DIF (Category A DIF), differences between 0.43 logits and 0.63 logits can be interpreted as slight-to-moderate DIF (Category B DIF), and differences equal to or greater than 0.64 logits as substantial DIF (Category C DIF). These recommendations reflect a conversion between the Delta index that ETS uses to report Mantel-Haenszel DIF statistics (Holland & Thayer, 1988) and logits, where one Delta unit is equal to 0.426 logits. 

In some cases, it may also be useful to use a statistical hypothesis test to evaluate whether the difference in item difficulty estimates between groups is larger than what could be expected by chance. Many researchers use the Rasch separate calibration t-test method (Wright & Stone, 1979). This test evaluates the difference between item difficulty estimates calculated from separate calibrations of the same item that have been equated to a common scale. The t-test is defined as:

-[] Add Equation 7.2 here

In Equation 7.2, $/delta_i1$ and $/delta_i2$ are defined as in Equation 7.1, and sp is the pooled variance, defined as:

-[] Add Equation 7.3 here

In Equation 7.3, s1 is the standard error of estimate for $/delta_i1$, s2 is the standard error of estimate for $/delta_i2$, n1 is the number of test-takers in Group 1, and n2 is the number of test-takers in Group 2 (Smith, 1996). The resulting t-statistic can be interpreted using a p-value.

When more than two groups are of interest, it is possible to apply the t-test method to compare pairs of groups (e.g., Group A compared to Group B; Group B compared to Group C; Group C compared to Group A). As needed, corrections such as Bonferroni adjustments can be used to reduce the chance of Type 1 errors from multiple comparisons.

***Interaction Analyses***

Another method for identifying DIF is to examine interactions between item difficulty and examinee subgroup identifiers using the Many-Facet Rasch Model (MFRM; Linacre, 1989; See Chapter 6). Including interaction terms in MFRMs allows researchers to test the null hypothesis that the calibrations of elements within one facet (e.g., items) are invariant across levels of another facet (e.g., subgroups). For the purpose of identifying DIF, researchers can examine the magnitude of the interaction effect between individual items and relevant examinee subgroups, where larger interaction terms indicate larger differences in item difficulty parameters for the subgroup of interest.

For example, the following formulation of the MFRM could be used to evaluate DIF:

-[] Insert Equation 7.4 here

In Equation 7.4, $\theta_n$ is the location of Examinee n, $\delta_i$ is the location of item i, $\gamma_g$ is the location of examinee subgroup G, and $\tau_k$ is the rating scale category threshold between category k and category k - 1. The model also includes the interaction between item locations and subgroup locations: $\delta_i$*$\gamma_g$. The interaction term tests the null hypothesis that item locations are invariant across examinee subgroups, against the alternative hypothesis that item locations are different across examinee subgroups, which would be evidence of DIF.

***Analysis of Variance (ANOVA) Methods***

Hagquist and Andrich (2017) discussed an approach to detecting uniform DIF and non-uniform DIF based on ANOVA of residuals. This approach is promising because it allows researchers to explore patterns of DIF related to more than two subgroups using a single analysis. This approach involves using a Rasch model 
to estimate examinee, item, and other location parameters of interest. Then, standardized residuals are analyzed using two-way independent analysis of variance (ANOVA) models with factors for examinee subgroups (e.g., demographic subgroups) and achievement level subgroups. Achievement level subgroups can be constructed using the empirical distribution of examinee location parameters. The ANOVA model is specified such that the standardized residuals for a given item are the dependent variable, and the independent variables (i.e., factors) include main effects for demographic subgroups and achievement-level subgroups, and the interaction between the demographic subgroups and achievement-level subgroups. 

The main effect for the demographic subgroup factor tests the null hypothesis that the average value of the standardized residuals for examinees in the demographic subgroups are equal, controlling for achievement level. The main effect for the achievement-level subgroup factor tests the null hypothesis that the average value of the standardized residuals was equal across achievement levels. The interaction effect between demographic subgroup and achievement-level subgroups tests the null hypothesis that the average value of the standardized residuals were equal across achievement levels. This effect reflects non-uniform DRF. For additional details about this procedure, we recommend that readers consult Hagquist and Andrich (2017).


## Detecting Differential Item Functioning in R

In the next section, we provide a step-by-step demonstration of the three approaches to DIF analyses that we described earlier using R. We encourage readers to use the example data set that is provided in the online supplement to conduct the analysis along with us.

### Example Data: Transitive Reasoning

The example data for this chapter is the Transitive Reasoning Data that we used in Chapter 2. The data include a group of 425 children's scored responses to assessment items designed to measure their ability to reason about the relationships among physical objects. Along with student responses, the Transitive Reasoning data file includes students' grade levels, ranging from Grade 2 to Grade 6. We will use students' grade level as the basis for our DIF analyses. Please see Chapter 2 for additional details about these data.

### Prepare for the Analyses: Install and Load Packages

We will use the *Extended Rasch Modeling* or *eRm* package (Mair et al., 2020) and the *Test Analysis Modules* or *TAM* package (Robitzsch et al., 2020) to demonstrate DIF analyses in this chapter. 

We will get started with these packages by viewing the citation information, and then installing and loading them into the R environment.

```{r}
citation("eRm")
# install.packages("eRm")
#install.packages("eRm")
library("eRm")

citation("TAM")
# install.packages("eRm")
#install.packages("eRm")
library("TAM")
```

### Getting Started

Now that we have installed and loaded the packages to our R session, we are ready to import the data. We will use the function *read.csv()* to import the comma-separated values (.csv) file that contains the Transitive Reasoning assessment data. We encourage readers to use their preferred method for importing data files into R or R Studio.

Please note that if you use *read.csv()* to import the data, you will need to specify the file path to the location at which the data file is stored on your computer or set your working directory to the folder in which you have saved the data.

First, we will import the data using *read.csv()* and store it in an object called *transreas*:

```{r}
transreas <- read.csv("transreas.csv")
```

Next, we need to identify the variable that contains examinees' subgroup membership identifiers for our DIF analysis. In our example analysis, we will conduct DIF analyses related to students' grade level. The Transitive Reasoning assessment data includes five grade levels (Grade 2 through Grade 6). For the sake of a simple illustration, we will re-code these grade levels into two groups: (1) Lower Elementary students (Grade 2 through Grade 4) and Upper Elementary students (Grade 5 and Grade 6).

The following code creates a vector object that includes students' membership in the Lower Elementary or Upper Elementary groups. Then, we use the *table()* function to generate a frequency table for this variable.

```{r}
transreas.grade.group <- ifelse(transreas$Grade <= 4, 1, 2)

table(transreas.grade.group)
```


### DIF Method 1: Compare Item Locations between Subgroups

First, we will conduct a DIF analysis by comparing the difficulty estimate of each Transitive Reasoning item between our two student subgroups. We will use the dichotomous Rasch model (see Chapter 2) to estimate item difficulty locations.

The eRm package includes a function that allows researchers to calculate group-specific item difficulty estimates *after* the item responses have been analyzed for the complete sample. Accordingly, we will begin our analysis by estimating item and person locations for the complete sample of students who completed the Transitive Reasoning assessment using the Rasch model.

To get started with the DIF analysis in eRm, we need to isolate the response matrix from our data and then apply the dichotomous Rasch model to the responses. The following code completes these steps. Please refer to Chapter 2 for a detailed description of procedures for applying and interpreting the results from the dichotomous Rasch model.

-[] Cheng, please omit output from this chunk.
```{r results="hide"}
## Isolate the response matrix:
transreas.responses <- subset(transreas, select = -c(Student, Grade))

## Apply the model to the data
transreas.RM <- RM(transreas.responses)
```

Next, we will calculate subgroup-specific item difficulty values using the *Waldtest()* function from eRm. We will specify the *transreas.grade.group* object as the grouping variable for our analysis.

```{r}
subgroup_diffs <- Waldtest(transreas.RM, splitcr = transreas.grade.group)
```

This analysis saves numerous details to the object called “subgroup_diffs”. Let’s create new objects with the group-specific item difficulties:

```{r}
subgroup_1_diffs <- subgroup_diffs$betapar1
subgroup_2_diffs <- subgroup_diffs$betapar2
```

We can examine the differences between the subgroup-specific item difficulties by subtracting the two sets of values.

```{r}
subgroup_1_diffs - subgroup_2_diffs
```
From these results, we can see that there are some items that were easier for Lower Elementary students (positive differences) and other items that were easier for Upper Elementary students (negative differences). In addition, the difference in logit-scale locations for two items (Task 2 and Task 6) exceed the "half-logit" rule, suggesting potentially meaningful DIF. The difference for Task 3 approximates this value as well.

To better understand the significance of differences in item difficulty between subgroups, we will examine the results from the statistical hypothesis test for the item difficulty comparisons. Let's create an object called "comparisons" in which we store these values.

```{r}
comparisons <- as.data.frame(subgroup_diffs$coef.table)
```

For reporting purposes, it is often helpful to construct a table that includes subgroup-specific item difficulty values along with the results from teh statistical hypothesis test. The following code saves these results in an object called *comparison.results*, and then prints the object to the console.

```{r}
comparison.results <- cbind.data.frame(subgroup_1_diffs, subgroup_diffs$se.beta1,
                                       subgroup_2_diffs, subgroup_diffs$se.beta2,
                                       comparisons)
# Name the columns of the results
names(comparison.results) <- c("Subgroup_1_Difficulty", "Subgroup_1_SE",
                               "Subgroup_2_Difficulty", "Subgroup_2_SE",
                               "Z", "p_value")
comparison.results
```

From these results, we see that there is one item with a statistically significant difference based on p < 0.05: Task 2.

As an additional interpretation aid, we will construct a scatterplot that shows the alignment between item difficlty estimates for the two subgroups in our analysis. Because the procedure that we used to calculate item difficulties specific to each subgroup has already adjusted the difficulties to be on the same scale, we don’t need to apply any transformation. The following code will create a scatterplot with 95% confidence bands to highlight items that are significantly different between the subgroups.

```{r}
## Calculate values for constructing the confidence bands:

mean.1.2 <- ((subgroup_1_diffs - mean(subgroup_1_diffs))/2*sd(subgroup_1_diffs) +
               (subgroup_2_diffs - mean(subgroup_2_diffs))/2*sd(subgroup_2_diffs))

joint.se <- sqrt((subgroup_diffs$se.beta1^2/sd(subgroup_1_diffs)) +
                   (subgroup_diffs$se.beta2^2/sd(subgroup_2_diffs)))

upper.group.1 <- mean(subgroup_1_diffs) + ((mean.1.2 - joint.se )*sd(subgroup_1_diffs))
upper.group.2 <- mean(subgroup_2_diffs) + ((mean.1.2 + joint.se )*sd(subgroup_2_diffs))

lower.group.1 <- mean(subgroup_1_diffs) + ((mean.1.2 + joint.se )*sd(subgroup_1_diffs))
lower.group.2 <- mean(subgroup_2_diffs) + ((mean.1.2 - joint.se )*sd(subgroup_2_diffs))

upper <- cbind.data.frame(upper.group.1, upper.group.2)
upper <- upper[order(upper$upper.group.1, decreasing = FALSE),]

lower <- cbind.data.frame(lower.group.1, lower.group.2)
lower <- lower[order(lower$lower.group.1, decreasing = FALSE),]

## Construct the scatterplot:

plot(subgroup_1_diffs, subgroup_2_diffs, xlim = c(-4, 4), ylim = c(-4, 4),
     xlab = "Lower Elementary", ylab = "Upper Elementary", main = "Lower Elementary Measures plotted against Upper Elementary Measures", cex.main = .8)
abline(a = 0, b = 1, col = "purple")

par(new = T)

lines(upper$upper.group.1, upper$upper.group.2, lty = 2, col = "red")

lines(lower$lower.group.1, lower$lower.group.2, lty = 2, col = "red")

legend("bottomright", c("Item Location", "Identity Line", "95% Confidence Band"),
       pch = c(1, NA, NA), lty = c(NA, 1, 2), col = c("black", "purple", "red"), cex = .8)
```

### DIF Method 2: Interaction Analysis

Next, we will conduct a DIF analysis with the Transitive Reasoning data using the interaction analysis approach based on the Many-Facet Rasch Model (MFRM; see Chapter 6). To do this, we will use the TAM package (Robitzsch et al., 2020) to specify a MFRM that includes an interaction term between item difficulty and student subgroup membership.

To get started with the interaction analysis with  the MFRM, we need to isolate the response matrix from our data and then specify the model. The following code completes these steps. Please refer to Chapter 6 for a detailed description of procedures for applying and interpreting the results from the MFRM.

```{r}
## Specify the facets in the model (grade-level subgroup)
facets <- as.data.frame(transreas.grade.group)
facets <- facets[,"transreas.grade.group", drop=FALSE]

## Identify the object of measurement (students)
students <- transreas$Student

## Identify the response matrix:
ratings <- transreas.responses

## Specify the model equation with the interaction term:
transreas_MFRM_equation <- ~ item + transreas.grade.group + (item*transreas.grade.group)

## Apply the model to the responses:
transreas_MFRM <- tam.mml.mfr(resp = ratings, facets = facets, formulaA = transreas_MFRM_equation, pid = students, constraint = "items", verbose = FALSE)
```

The facet estimates from the MFRM include values for each item, subgroup, and the interaction between each item and subgroup. We will store all of the facet estimates in an object called *facet.estimates*, and then store the results for each facet in separate objects for easier manipulation.

```{r}
facet.estimates <- transreas_MFRM$xsi.facets

item.estimates <- subset(facet.estimates, facet.estimates$facet == "item")
subgroup.estimates <- subset(facet.estimates, facet.estimates$facet == "transreas.grade.group")
interaction.estimates <- subset(facet.estimates, facet.estimates$facet == "item:transreas.grade.group")
```

For the current analysis, our main focus is on the interaction results. We can examine the relative magnitude of each interaction term by plotting the values in a simple scatterplot. The first ten values in the *interaction.estimates* object show the interaction between each item and group 1 (Lower Elementary), and the second ten values show the interaction between each item and group 2 (Upper Elementary). We will construct separate plots for each subgroup.

```{r}
plot(interaction.estimates$xsi[1:10], main = "Interaction Effects for Lower Elementary Students", ylab = "Interaction Estimate in Logits", xlab = "Item Number")
```

```{r}
plot(interaction.estimates$xsi[11:20], main = "Interaction Effects for Upper Elementary Students", ylab = "Interaction Estimate in Logits", xlab = "Item Number")
```
The interaction estimates are reported on the same logit scale as the item and person estimates. Values equal to zero indicate no interaction between item difficulty and subgroup membership. Positive values indicate that an item was relatively easier for a given subgroup compared to its overall location estimate. Negative values indicate that an item was relatively more difficult for a given subgroup compared to its overall location estimate.

We can examine the results and see that the largest interaction terms were observed for Task 2 and Task 6. Task 2 was more difficult for Lower Elementary students compared to Upper Elementary students. The opposite pattern was true for Task 6, which was easier for Lower Elementary Students compared to Upper Elementary students.

### DIF Method 3: Analysis of Variance

Next, we will conduct a DIF analysis with the Transitive Reasoning data using an Analysis of Variance (ANOVA) of standardized residuals (Hagquist & Andrich, 2017). We will use the dichotomous Rasch model to estimate the item and person parameters that are necessary to calculate standardized residuals. 

Our ANOVA model will include factors for students' grade-level subgroups (Lower Elementary and Upper Elementary), and achievement level subgroups. To create the achievement level subgroups, we used the empirical distribution of the student achievement parameters to create three subgroups with approximately equal sample sizes. In other applications, researchers may choose to use fewer or more achievement-level subgroups. After we estimate student achievement parameters, we will use the *ntile()* function from the *dplyr* package (Wickham et al., 2021) to construct groups based on the distribution of student  achievement estimates. 

```{r}
student.locations <- person.parameter(transreas.RM)
achievement <- student.locations$theta.table

#install.packages("dplyr")
library(dplyr)

achievement$group <- ntile(achievement$`Person Parameter`, 3)
```

We use the *table()* function to construct a frequency table that shows the number of students in each achievement-level subgroup.
```{r}
table(achievement$group)
```

Next, we need to calculate the standardized residuals for each student's response to each item. These residuals will act as the dependent variable in our ANOVA models. We can calculate standardized residuals using the *itemfit()* function from eRm (see Chapter 3 for more details). We save the results in an object called *std.resids*.

```{r}
item.fit <- itemfit(student.locations)
std.resids <- as.data.frame(item.fit$st.res)
```

Next, we will isolate the residuals specific to each item and use ANOVA to identify potential DIF. For the sake of illustration, we will demonstrate this procedure with Item 1 first. Then, we will use a for-loop to apply the same procedure to all of the items.

To get started, we will create an object called *item.resids* that includes the standardized residuals for Item 1.
```{r}
item.resids <- std.resids[, 1]
```

Next, we will run an ANOVA using the *aov()* function from R. In our model, standardized residuals are the dependent variable, and the factors include student grade-level subgroup, achievement-level subgroup, and the interaction between grade-level subgroups and achievement-level subgroups.

To do this, we need to exclude students for whom we do not have standardized residuals as a result of constant responses. We specify the *grade_level* and *theta_group* to exclude the students who were not included in the estimation procedure as a result of constant responses.
   
```{r}       
grade_level <- transreas.grade.group[-c(student.locations$pers.ex)]
theta_group <- achievement$group[-c(student.locations$pers.ex)]
```

Next, we run our ANOVA model and save the results in an object called *item.anova*. We use the *summary()* function to examine the results.

```{r} 
item.anova <- aov(item.resids ~ grade_level + theta_group + (grade_level*theta_group), data = std.resids)

summary(item.anova)
```
  
From these results, we can see that all of the F statistics are relatively small. Accordingly, we can conclude that there is no substantial evidence of DIF associated with Task 1.

Next, we will run the same model on the standardized residuals for Task 2.

```{r}
item.resids <- std.resids[, 2]

item.anova <- aov(item.resids ~ grade_level + theta_group + (grade_level*theta_group), data = std.resids)

summary(item.anova)
```

For task 2, we see a slightly larger effect for the grade_level factor, with p = 0.10. 

Now we will repeat this process for all of the tasks. For the sake of brevity in the print book, we only show the results for the first three items. Readers can modify the *selected.items* statement at the beginning of the code chunk to complete this process for all of the items or for a different set of items.

```{r}
selected.items <- c(1:3)

for(item.number in selected.items){
 
  item.resids <- std.resids[, item.number]

  item.anova <- aov(item.resids ~ grade_level + theta_group + (grade_level*theta_group), data = std.resids)

  print(paste("Item", item.number))
  print(summary(item.anova))
}
```





### DIF Analysis with the Partial Credit Model

We will practice running some DIF analyses based on the Partial Credit model. To do this, we will use the “pcmdat2” data that are provided as part of the eRm package. Most of the procedures for this analysis mirror those from Part One. As a result, less detail will be provided in this part of the lab except where needed to highlight differences.

```{r}
### Load & summarize the example data:
data("pcmdat2")
summary(pcmdat2)
```

Analyze the data using the Partial Credit model, and store the results in an object called “PC_model”

```{r}
### Analyze the data using the Partial Credit model:
PC_model <- PCM(pcmdat2)
```

### Conduct DIF analyses

NOTE! The estimation procedure for polytomous (> 2 categories) data procedures estimates of rating scale category thresholds for each item. If you want to compare the threshold locations between subgroups, you can use: `subgroup_diffs <- Waldtest(PC_model, splitcr = subgroups)` to generate the DIF results specific to item-category thresholds, and then proceed as in Part two.

However, if you want to make comparisons at the overall item level, you’ll need to get item difficulty estimates and standard errors for the overall item. You can do this using the following code.

```{r}
### DIF analysis

# Create subgroup classifications:
subgroups <- rep(c(1,2), nrow(pcmdat2)/2)

# Calculate subgroup-specific item difficulty values (#select/highlight code starting here):

#- First, get overall item difficulties specific to each subgroup:

group1_item.diffs.overall <- NULL
group2_item.diffs.overall <- NULL

responses <- pcmdat2 # update this if needed with the responses object
model.results <- PC_model # update this if needed with the model results object

responses.g <- cbind.data.frame(subgroups, responses)
responses.g1 <- subset(responses.g, responses.g$subgroups == 1)
responses.g2 <- subset(responses.g, responses.g$subgroups == 2)

## Compare thresholds between groups:
subgroup_diffs <- Waldtest(PC_model, splitcr = subgroups)


for(item.number in 1:ncol(responses)){
  
  n.thresholds.g1 <-  length(table(responses.g1[, item.number+1]))-1
  
  group1_item.diffs.overall[item.number] <- mean(subgroup_diffs$betapar1[((item.number*(n.thresholds.g1))-(n.thresholds.g1-1)): 
                                                                           (item.number*(n.thresholds.g1))])*-1
  
  n.thresholds.g2 <-  length(table(responses.g2[, item.number+1]))-1
  
  group2_item.diffs.overall[item.number] <- mean(subgroup_diffs$betapar2[((item.number*(n.thresholds.g2))-(n.thresholds.g2-1)): 
                                                                           (item.number*(n.thresholds.g2))])*-1
}

group1_item.diffs.overall
group2_item.diffs.overall

## Get overall item SE values:

#- First, get overall SEs specific to each subgroup:

group1_item.se.overall <- NULL
group2_item.se.overall <- NULL

responses <- pcmdat2 # update this if needed with the responses object
model.results <- PC_model # update this if needed with the model results object

responses.g <- cbind.data.frame(subgroups, responses)
responses.g1 <- subset(responses.g, responses.g$subgroups == 1)
responses.g2 <- subset(responses.g, responses.g$subgroups == 2)

subgroup_diffs <- Waldtest(PC_model, splitcr = subgroups)


for(item.number in 1:ncol(responses)){
  
  n.thresholds.g1 <-  length(table(responses.g1[, item.number+1]))-1
  
  group1_item.se.overall[item.number] <- mean(subgroup_diffs$se.beta1[((item.number*(n.thresholds.g1))-(n.thresholds.g1-1)): 
                                                                        (item.number*(n.thresholds.g1))])
  
  n.thresholds.g2 <-  length(table(responses.g2[, item.number+1]))-1
  
  group2_item.se.overall[item.number] <- mean(subgroup_diffs$se.beta2[((item.number*(n.thresholds.g2))-(n.thresholds.g2-1)): 
                                                                        (item.number*(n.thresholds.g2))])
}

group1_item.se.overall
group2_item.se.overall

```

Then, we will use our own code to calculate test statistics for the differences in overall item difficulties:
  
```{r}
# Calculate test statistics for item comparisons:
z <- (group1_item.diffs.overall - group2_item.diffs.overall)/
  sqrt(group1_item.se.overall^2 + group2_item.se.overall^2)

# view test statistics in the console:
z
```

Create scatterplot of item measures with 95% confidence bands

```{r}
### DIF Scatterplot:

## First, calculate values for constructing the confidence bands:

mean.1.2 <- ((group1_item.diffs.overall - mean(group1_item.diffs.overall))/2*sd(group1_item.diffs.overall) +
               (group2_item.diffs.overall - mean(group2_item.diffs.overall))/2*sd(group2_item.diffs.overall))

joint.se <- sqrt((group1_item.se.overall^2/sd(group1_item.diffs.overall)) +
                   (group2_item.se.overall^2/sd(group2_item.diffs.overall)))


upper.group.1 <- mean(group1_item.diffs.overall) + ((mean.1.2 - joint.se )*sd(group1_item.diffs.overall))
upper.group.2 <- mean(group2_item.diffs.overall) + ((mean.1.2 + joint.se )*sd(group2_item.diffs.overall))

lower.group.1 <- mean(group1_item.diffs.overall) + ((mean.1.2 + joint.se )*sd(group1_item.diffs.overall))
lower.group.2 <- mean(group2_item.diffs.overall) + ((mean.1.2 - joint.se )*sd(group1_item.diffs.overall))


upper <- cbind.data.frame(upper.group.1, upper.group.2)
upper <- upper[order(upper$upper.group.1, decreasing = FALSE),]


lower <- cbind.data.frame(lower.group.1, lower.group.2)
lower <- lower[order(lower$lower.group.1, decreasing = FALSE),]

## make the scatterplot:

plot(group1_item.diffs.overall, group2_item.diffs.overall, xlim = c(-3, 3), ylim = c(-3, 3),
     xlab = "Group 1", ylab = "Group 2", main = "Group 1 Measures \n plotted against \n Group 2 Measures")
abline(a = 0, b = 1, col = "purple")

par(new = T)

lines(upper$upper.group.1, upper$upper.group.2, lty = 2, col = "red")

lines(lower$lower.group.1, lower$lower.group.2, lty = 2, col = "red")

legend("bottomright", c("Item Location", "Identity Line", "95% Confidence Band"),
       pch = c(1, NA, NA), lty = c(NA, 1, 2), col = c("black", "purple", "red"))

```


## Example APA-format results section for basic DIF analysis

In this analysis, we examined the degree to which there was evidence of uniform differential item functioning (DIF) between two subgroups of participants (group 1 and group 2) in the “pcmdat2” example dataset from the using the Extended Rasch Models (“eRm”) package (Mair, Hatzinger, & Maier, 2020) for R. The data included 300 participants’ responses to four rating scale items made up of three ordered categories (*X* = 0, 1, or 2). We used the eRm package to conduct all of the analyses.

As a first step in the analysis, we examined item difficulty estimates and standard errors specific to each subgroup using the Partial Credit model (Masters, 1982). We estimated item difficulty for the two subgroups using a combined analysis of both subgroups and then estimating the group-specific item difficulties using the eRm package. Group-specific item difficulties and standard errors are shown in Table 1.

To examine the differences in item difficulty between subgroups, we calculated standardized differences following Wright and Masters (1982) as follows: $$z=\left(d_{1}-d_{2}\right) / \sqrt{se_{1}^{2}+se_{2}^{2}}$$
  
  where $z$ is the standardized difference, d1 is the item difficulty specific to Subgroup 1, d2 is the item difficulty specific to Subgroup 2, $se_{1}^{2}$ is the standard error of the item difficulty specific to Subgroup 1, and $se_{2}^{2}$ is the standard error of the item difficulty specific to Subgroup 2. Using the formulation of the z statistic, higher values of z indicate higher item locations (more-difficult to endorse) for Subgroup 1 compared to Subgroup 2.   

Figure 1 shows a plot of the z-statistics for the four survey items; these values are also presented numerically in Table 1. In the figure, the x-axis shows the item identification numbers, and the y-axis shows the value of the z-statistic. Boundaries at +2 and -2 are indicated using dashed horizontal lines to highlight statistically significant differences in item difficulty between subgroups. Examination of these results indicates that the items were not significantly different in difficulty between the two subgroups. In addition, there were both positive and negative z statistics, indicating that although the differences in item difficulty were not significant, there were some items that were easier to endorse for Subgroup 1 and others that were easier to endorse for Subgroup 2.

![Figure 1: Plot of Standardized Differences for Items between Subgroups](DIF_figure1.png)  

To further explore the differences in item difficulty between the two subgroups, Figure 2 shows a scatterplot of the item locations between the two subgroups. In the plot, the item difficulty for Subgroup 1 is shown on the x-axis, and the item difficulty for Subgroup 2 is shown on the y-axis. Individual items are indicated using open circle plotting symbols. A solid identity line is included to highlight deviations from invariant item difficulties between the two groups: Points that fall below this line indicate that items were easier to endorse (lower item measures) for Subgroup 2, and points that fall above the line indicate that items were easier to endorse (lower item measures) for Subgroup 1. Dashed lines are also included to indicate a 95% confidence interval for the difference between the item measures, following Luppescu (1995).

![Figure 2: Scatterplot of Subgroup-Specific Item Difficulties](DIF_figure2.png)

Finally, Figure 3 is a bar plot that illustrates the direction and magnitude of the differences in item difficulty between subgroups. In the plot, each bar represents the difference in difficulty between subgroups for an individual item, ordered by the item sequence in the survey. Bars that point to the left of the plot indicate that the item was easier to endorse for Subgroup 1, and bars that point to the right of the plot indicate that the item was easier to endorse for Subgroup 2. Dashed vertical lines are plotted that show values of +0.5 and -0.5 logits as an indicator of substantial differences in item difficulty between subgroups. 

![Figure 3: Bar Plot of Differences In Item Difficulty Between Subgroups](DIF_figure3.png)  

## References

Luppescu, S. (1995). Comparing measures: Scatterplots. Rasch Measurement Transactions, 9(1), 410.

Masters, G. N. (1982). A Rasch model for partial credit scoring. Psychometrika, 47(2), 149–174. https://doi.org/10.1007/BF02296272

Masters, G. N., & Wright, B. D. (1997). The partial credit model. In W. J. van der Linden & R. K. Hambleton (Eds.), Handbook of modern item response theory (pp. 101–121). Springer.

Mair, P., Hatzinger, R., & Maier M. J. (2020). eRm: Extended Rasch Modeling. 1.0-1. https://cran.r-project.org/package=eRm

Wright, B. D., & Masters, G. N. (1982). Rating Scale Analysis: Rasch Measurement. MESA Press.
