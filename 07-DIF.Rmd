# Basics of Differential Item Functioning {#DIF}

This chapter provides a basic overview of differential item functioning (DIF) along with methods for detecting DIF with Rasch models using R. We will use the eRm package (Mair et al., 2020) and the TAM package (Robitzch et al., 2020) for the analyses in this chapter.

## What is Differential Item Functioning (DIF)?

DIF occurs when examinees who are members of different groups (e.g., demographic subgroups) who have the *same location* on the latent variable have *different* probabilities for a response in a given category (e.g., a correct response or a rating of "agree") on an item. In the context of Rasch measurement theory (Rasch, 1960), DIF means that an item has a different location (i.e., a different level of difficulty) across subgroups by more than the standard error. Rasch analyses of DIF typically focus on detecting *uniform DIF*, which occurs when the distance between item response functions (IRFs) is constant between subgroups.

Figure 7.1 illustrates uniform DIF between two groups of examinees for a dichotomous item (x = 0, 1). The x-axis shows examinee locations on the latent variable, and the y-axis shows the probability for a correct or positive response (P(x=1)). The dashed line shows the IRF for Group 1, and the dashed line shows IRF for Group 2. This item shows DIF because examinees who have the same location on the latent variable have different probabilities for a correct or positive response.

Figure 7.1
```{r}
IRF <- function(b_group1, b_group2){
  colors <- c("purple", "seagreen3")
  theta <- seq(-3, 3, .1)
  P1 <- 1 / (1 + exp(-(theta - b_group1)))
  P2 <- 1 / (1 + exp(-(theta - b_group2)))
  plot(theta, P1, type="l", xlim=c(-3,3), ylim=c(0,1),
       xlab="Theta", ylab="P(X=1)", col = colors[1], lty = 1,
       main = "Item i")
  lines(theta, P2, col = colors[2], lty = 2)
  legend("bottomright", c("Group 1", "Group 2"), lty = c(1, 2),
         col = c(colors[1], colors[2]))
}

IRF(b_group1 = 1, b_group2 = -1)
```

DIF analysis in modern measurement theory in general, as well as in the context of Rasch models in particular, is nuanced. The interpretation and use of DIF indices varies along with the individual purpose and consequences of each assessment. As a result, it is critical that analysts consider the unique context in which they are evaluating DIF when they interpret the results from DIF analyses. In addition, it is often useful to consider DIF from multiple perspectives, including different statistical techniques for identifying DIF. 

It is also important to note that when DIF occurs, it does not always imply a threat to fairness from a psychometric perspective (AERA et al., 2014). Instead, evidence of DIF is a *starting place* for additional research, such as qualitative analyses of item content or the assessment context, to evaluate whether DIF may reflect a threat to fairness. 

In this chapter, we demonstrate techniques that analysts can use to identify DIF within the Rasch measurement theory framework. Our presentation is not exhaustive and there are many other methods that can be used to supplement those that we demonstrate here. We encourage interested readers to consult the resources listed throughout the chapter, as well as the resource list at the end of this chapter, to learn more about identifying DIF within the context of Rasch measurement theory.

## Methods for Identifying DIF with Rasch models

From the perspective of Rasch measurement theory, DIF poses a serious threat to measurement quality because it implies a lack of invariance. When DIF occurs, the requirements for invariant measurement are not met (see Chapter 1). Accordingly, DIF can be interpreted as not only a potential threat to fairness, but also as a violation of Rasch model requirements (Hagquist & Andrich, 2017).

Researchers have proposed several methods for identifying DIF within the framework of Rasch measurement theory. Methods that are routinely used in practical applications of Rasch measurement theory can be broadly classified into three approaches: (1) Comparison of group-specific item estimates; (2) Interaction analyses between item locations and examinee subgroup indicators; and (3) Methods based on analysis of variance with residuals. There are numerous other approaches to detecting DIF that are beyond the scope of this volume. We encourage readers who are interested in learning more about DIF to consult the resources that we have listed at the end of this chapter.

***Comparing Group-Specific Item Estimates***

A popular method for identifying DIF within the framework of Rasch measurement theory is to calculate group-specific estimates of item difficulty, and compare those estimates to see if they are meaningfully different. In order to meaningfully compare item difficulty estimates between two groups, it is necessary to adjust (i.e., equate) the item difficulty measures for one group so that they are on the same scale as the item difficulty measures for the second group. The equated differences in item difficulty estimates  reflect the distance between group-specific IRFs (Raju, 1988).

Some software programs, including the eRm package (Mair et al., 2020), perform this adjustment procedure automatically as part of their DIF functions. In other cases, the analyst needs to perform the adjustment procedure manually before item locations can be compared. For a discussion of the adjustment procedure, please see Luppescu (1995).

After group-specific item difficulty estimates have been calculated and adjusted for the purpose of comparison, it is necessary to consider the degree to which differences in item difficulty estimates are large enough to indicate substantial DIF. Researchers who use Rasch models to evaluate DIF evaluate these differences using several approaches. For example, many researchers calculate the difference between item difficulty estimates as:

-[] Add Equation 7.1 here

In Equation 7.1, $/delta_i1$ is the group-specific item difficulty estimate for Group 1, and $/delta_i2$ is the group-specific item difficulty estimate for Group 2. Researchers sometimes interpret the difference in item difficulty in logits as an effect size, where larger differences indicate more substantial DIF. Wright and Douglas (1975) recommended that researchers use a critical value of 0.5 logits (i.e., the "half-logit rule") to identify DIF that warrants further investigation. This recommendation is based on the typical range of item difficulty for achievement tests (around -2.5 logits to around 2.5 logits), because a difference of 0.5 logits reflects 10% of the item difficulty scale. Differences of this magnitude may impact the accuracy of the measurement procedure between subgroups. Other researchers have proposed smaller critical values, such as Engelhard and Myford (2003), who recommended a difference of 0.3 logits to identify meaningful DIF. 

In Winsteps software manual, Linacre (2020) proposed guidelines for interpreting differences in item difficulty that reflect the *Delta Index* from ETS (Dorans & Holland, 1993). He noted that differences less than 0.43 logits can be interpreted as negligible DIF (Category A DIF), differences between 0.43 logits and 0.63 logits can be interpreted as slight-to-moderate DIF (Category B DIF), and differences equal to or greater than 0.64 logits as substantial DIF (Category C DIF). These recommendations reflect a conversion between the Delta index that ETS uses to report Mantel-Haenszel DIF statistics (Holland & Thayer, 1988) and logits, where one Delta unit is equal to 0.426 logits. 

In some cases, it may also be useful to use a statistical hypothesis test to evaluate whether the difference in item difficulty estimates between groups is larger than what could be expected by chance. Many researchers use the Rasch separate calibration t-test method (Wright & Stone, 1979). This test evaluates the difference between item difficulty estimates calculated from separate calibrations of the same item that have been equated to a common scale. The t-test is defined as:

-[] Add Equation 7.2 here

In Equation 7.2, $/delta_i1$ and $/delta_i2$ are defined as in Equation 7.1, and sp is the pooled variance, defined as:

-[] Add Equation 7.3 here

In Equation 7.3, s1 is the standard error of estimate for $/delta_i1$, s2 is the standard error of estimate for $/delta_i2$, n1 is the number of test-takers in Group 1, and n2 is the number of test-takers in Group 2 (Smith, 1996). The resulting t-statistic can be interpreted using a p-value.

When more than two groups are of interest, it is possible to apply the t-test method to compare pairs of groups (e.g., Group A compared to Group B; Group B compared to Group C; Group C compared to Group A). As needed, corrections such as Bonferroni adjustments can be used to reduce the chance of Type 1 errors from multiple comparisons.

***Interaction Analyses***

Another method for identifying DIF is to examine interactions between item difficulty and examinee subgroup identifiers using the Many-Facet Rasch Model (MFRM; Linacre, 1989; See Chapter 6). Including interaction terms in MFRMs allows researchers to test the null hypothesis that the calibrations of elements within one facet (e.g., items) are invariant across levels of another facet (e.g., subgroups). For the purpose of identifying DIF, researchers can examine the magnitude of the interaction effect between individual items and relevant examinee subgroups, where larger interaction terms indicate larger differences in item difficulty parameters for the subgroup of interest.

For example, the following formulation of the MFRM could be used to evaluate DIF:

-[] Insert Equation 7.4 here

In Equation 7.4, $\theta_n$ is the location of Examinee n, $\delta_i$ is the location of item i, $\gamma_g$ is the location of examinee subgroup G, and $\tau_k$ is the rating scale category threshold between category k and category k - 1. The model also includes the interaction between item locations and subgroup locations: $\delta_i$*$\gamma_g$. The interaction term tests the null hypothesis that item locations are invariant across examinee subgroups, against the alternative hypothesis that item locations are different across examinee subgroups, which would be evidence of DIF.

***Analysis of Variance (ANOVA) Methods***

Andrich and Hagquist (2017) discussed an approach to detecting uniform DIF and non-uniform DIF based on ANOVA of residuals. This approach is promising because it allows researchers to explore patterns of DIF related to more than two subgroups using a single analysis. This approach involves using a Rasch model 
to estimate examinee, item, and other location parameters of interest. Then, standardized residuals are analyzed using two-way independent analysis of variance (ANOVA) models with factors for examinee subgroups (e.g., demographic subgroups) and achievement level subgroups. Achievement level subgroups can be constructed using the empirical distribution of examinee location parameters. The ANOVA model is specified such that the standardized residuals for a given item are the dependent variable, and the independent variables (i.e., factors) include main effects for demographic subgroups and achievement-level subgroups, and the interaction between the demographic subgroups and achievement-level subgroups. 

The demographic subgroup main effect tests the null hypothesis that the average value of the standardized residuals for students in the demographic subgroups were equal, controlling for achievement level. The achievement-level subgroup factor tests the null hypothesis that the average value of the standardized residuals was equal across achievement levels. The main effects test for uniform DIF.
The interaction effect between demographic subgroup and achievement-level subgroups tests the null hypothesis that the average value of the standardized residuals were equal across achievement levels. This effect reflects non-uniform DRF. For additional details about this procedure, we recommend that readers consult Andrich and Hagquist (2017).

## Example Data: Liking for Science with Student Subgroups

The example data for this chapter is the Liking For Science attitude survey data that we used in Chapter 4 and Chapter 5. The data include a group of 75 children's responses to the 25-item *Liking for Science* questionnaire, which was designed to measure their attitudes toward science activities. The data were published in Wright and Masters (1982). Each item stem included a science activity, and three response options: 0 = *Dislike*, 1 = *Not Sure/Don't Care*, and 2 = *Like*, such that responses in higher categories indicated more-favorable attitudes toward science activities.

The Liking for Science data file for the current chapter includes demographic subgroup identification numbers for students related to their gender. In the current analysis, there are two gender subgroups: 1 = Female; and 2 = Male.

### Prepare for the Analyses: Install and Load Packages

We will use the *Extended Rasch Modeling*, or *eRm* package (Mair et al., 2020) and the *Test Analysis Modules* or *TAM* package (Robitzsch et al., 2020) to demonstrate DIF analyses in this chapter. 

We will get started with these packages by viewing the citation information, and then installing and loading them into the R environment.

```{r}
citation("eRm")
# install.packages("eRm")
#install.packages("eRm")
library("eRm")

citation("TAM")
# install.packages("eRm")
#install.packages("eRm")
library("TAM")
```

### Getting Started

Now that we have installed and loaded the packages to our R session, we are ready to import the data. We will use the function *read.csv()* to import the comma-separated values (.csv) file that contains the Liking for Science survey data. We encourage readers to use their preferred method for importing data files into R or R Studio.

Please note that if you use *read.csv()* to import the data, you will need to specify the file path to the location at which the data file is stored on your computer or set your working directory to the folder in which you have saved the data.

First, we will import the data using *read.csv()* and store it in an object called *science*:

```{r}
science <- read.csv("liking_for_science.csv")
```

Next, we need to import a 

Next, we will explore the data using descriptive statistics using the *summary()* function:





Now, let's load the data. We will practice running some DIF analyses based on the dichotomous Rasch model. To do this, we will use the “raschdat1” data that are provided as part of the "eRm" package.

```{r}
### Load & summarize the example data:
data("raschdat1")
summary(raschdat1)
```

### Run dichotomous Rasch model

```{r}
### Analyze the data using the dichotomous Rasch model:
dichot_model <- RM(raschdat1)
summary(dichot_model)
```

### Identify subgroups

Then,we need to identify the subgroups between which you will examine DIF.
Since our data are made up, so we will make up subgroup classifications as well. We will do this by sampling 100 observations with replacement from a vector made up of the numbers “1” and “2” to match our 100-person data.

```{r}
# Create subgroup classifications:
subgroups <- sample(1:2, 100, replace = TRUE)
```

Then we will calculate subgroup-specific item difficulty values using the Waldtest() function from "eRm".

```{r}
# Calculate subgroup-specific item difficulty values:
subgroup_diffs <- Waldtest(dichot_model, splitcr = subgroups)
```

This analysis saves numerous details to the object called “subgroup_diffs”. Let’s create new objects with the group-specific item difficulties:

```{r}
# Create objects for subgroup-specific item difficulties:
subgroup_1_diffs <- subgroup_diffs$betapar1
subgroup_2_diffs <- subgroup_diffs$betapar2
```

#### Examine test-statistic result for the item comparisons

First, let's examine the test statistics & p-values for the item comparisons.
Let's create an object called "comparisons" in which we store the results:

```{r}
#store results from item comparisons in an object called "comparisons"
comparisons <- as.data.frame(subgroup_diffs$coef.table)
```

- You can view the “comparisons” object by clicking on it in your Environment pane in the upper right corner of R Studio.

- Once you click on “comparisons” you will see a preview of the comparison results.

- Sorting the results by p-value by clicking the arrow in the top right corner of the “p-value” column until the values are sorted from low to high.

- From these results, we see that there is one item with a statistically significant difference based on p < 0.05; Item 20 and Item 8.

### Visualize the test statistics for the item comparison

It is often useful to visualize the test statistics for the item comparison. You can do this using a simple scatterplot. Ti.	To run the scatterplot code all at once, highlight/select the code between “#*start” and “#* stop” and run it.

```{r}
graphics.off()
min.y <- ifelse(ceiling(min(comparisons$`z-statistic`)) > -3, -3, 
                ceiling(min(comparisons$`z-statistic`)))

max.y <- ifelse(ceiling(max(comparisons$`z-statistic`)) < 3, 3, 
                ceiling(max(comparisons$`z-statistic`)))

plot(comparisons$`z-statistic`, ylim = c(min.y, max.y),
     ylab = "Z", xlab = "Item", main = "Test Statistics for Item Comparisons \nbetween Subgroup 1 and Subgroup 2")
abline(h=2, col = "red", lty = 2)
abline(h=-2, col = "red", lty = 2)

legend("topright", c("Z Statistic", "Boundaries for Significant Difference"),
       pch = c(1, NA), lty = c(NA, 2), col = c("black", "red"), cex = .7)
```

This plot highlights items that are significantly different between subgroups.

### Export the item comparison results for later use using the following code.

```{r}
# Take out the values that we need for our comparison table
comparison.results <- cbind.data.frame(c(1:length(subgroup_1_diffs)),
                                       subgroup_1_diffs, subgroup_diffs$se.beta1,
                                       subgroup_2_diffs, subgroup_diffs$se.beta2,
                                       comparisons)
# Name the columns of the results
names(comparison.results) <- c("Item",
                             "Subgroup_1_Difficulty", "Subgroup_1_SE",
                               "Subgroup_2_Difficulty", "Subgroup_2_SE",
                               "Z", "p_value")
# Put the result into a csv file
write.csv(comparison.results, file = "comparison_results.csv")

```

### Plot the item difference

Now, let's make a scatterplot of the item differences for the two subgroups.

*NOTE:* Because the procedure that we used to calculate item difficulties specific to each subgroup has already adjusted the difficulties to be on the same scale, we don’t need to apply any transformation.

The following code will create a scatterplot with 95% confidence bands to highlight items that are significantly different between the subgroups.

```{r}
### DIF Scatterplots:

## First, calculate values for constructing the confidence bands:

mean.1.2 <- ((subgroup_1_diffs - mean(subgroup_1_diffs))/2*sd(subgroup_1_diffs) +
               (subgroup_2_diffs - mean(subgroup_2_diffs))/2*sd(subgroup_2_diffs))

joint.se <- sqrt((subgroup_diffs$se.beta1^2/sd(subgroup_1_diffs)) +
                   (subgroup_diffs$se.beta2^2/sd(subgroup_2_diffs)))


upper.group.1 <- mean(subgroup_1_diffs) + ((mean.1.2 - joint.se )*sd(subgroup_1_diffs))
upper.group.2 <- mean(subgroup_2_diffs) + ((mean.1.2 + joint.se )*sd(subgroup_2_diffs))

lower.group.1 <- mean(subgroup_1_diffs) + ((mean.1.2 + joint.se )*sd(subgroup_1_diffs))
lower.group.2 <- mean(subgroup_2_diffs) + ((mean.1.2 - joint.se )*sd(subgroup_2_diffs))


upper <- cbind.data.frame(upper.group.1, upper.group.2)
upper <- upper[order(upper$upper.group.1, decreasing = FALSE),]


lower <- cbind.data.frame(lower.group.1, lower.group.2)
lower <- lower[order(lower$lower.group.1, decreasing = FALSE),]

## make the scatterplot:

plot(subgroup_1_diffs, subgroup_2_diffs, xlim = c(-2, 2), ylim = c(-2, 2),
     xlab = "Group 1", ylab = "Group 2", main = "Group 1 Measures \n plotted against \n Group 2 Measures")
abline(a = 0, b = 1, col = "purple")

par(new = T)

lines(upper$upper.group.1, upper$upper.group.2, lty = 2, col = "red")

lines(lower$lower.group.1, lower$lower.group.2, lty = 2, col = "red")

legend("bottomright", c("Item Location", "Identity Line", "95% Confidence Band"),
       pch = c(1, NA, NA), lty = c(NA, 1, 2), col = c("black", "purple", "red"))

```

Finally, let’s make a bar plot to illustrate the differences in item difficulty between the subgroups.

```{r}
### Bar plot of item differences:

# First, calculate difference in difficulty between subgroups
# Note that I multiplied by -1 to reflect item difficulty rather than easiness (eRm quirk):
item_dif <- (subgroup_1_diffs*-1)-(subgroup_2_diffs*-1)


# Code to use different colors to highlight items with differences >= .5 logits:
colors <- NULL

for (item.number in 1:30){
  
  colors[item.number] <- ifelse(abs(item_dif[item.number]) > .5, "dark blue", "light green")
  
}

# Bar plot code:
item_dif <- as.vector(item_dif)

x <- barplot(item_dif, horiz = TRUE, xlim = c(-2, 2), 
             col = colors,
             ylim = c(1,40), 
             xlab = "Logit Difference")

# code to add labels to the plot:

dif_labs <- NULL

for (i in 1:length(subgroup_1_diffs)) {
  dif_labs[i] <- ifelse(item_dif[i] < 0, item_dif[i] - .2,
                        item_dif[i] + .2)
}

text(dif_labs, x, labels = c(1:length(subgroup_1_diffs)),
     xlim = c(-1.5, 1.5), cex = .8)

# add vertical lines to highlight .5 logit differences:
abline(v = .5, lty = 3)
abline(v = -.5, lty = 3)

# add additional text to help with interpretation:

text(-1, 40, "Easier to Endorse for Group 1", cex = .8)
text(1, 40, "Easier to Endorse for Group 2", cex = .8)

legend("bottomright", c("Diff >= .5 logits", "Diff < .5 logits"),
       pch = 15, col = c("dark blue", "light green"), cex = .7 )

```

## R-lab 2: DIF Analysis with the Partial Credit Model

We will practice running some DIF analyses based on the Partial Credit model. To do this, we will use the “pcmdat2” data that are provided as part of the eRm package. Most of the procedures for this analysis mirror those from Part One. As a result, less detail will be provided in this part of the lab except where needed to highlight differences.

### Load the data into your working environment and generate a summary of responses

```{r}
### Load & summarize the example data:
data("pcmdat2")
summary(pcmdat2)
```

### Run the Partial Credit Model

Analyze the data using the Partial Credit model, and store the results in an object called “PC_model”

```{r}
### Analyze the data using the Partial Credit model:
PC_model <- PCM(pcmdat2)
```

### Conduct DIF analyses

NOTE! The estimation procedure for polytomous (> 2 categories) data procedures estimates of rating scale category thresholds for each item. If you want to compare the threshold locations between subgroups, you can use: `subgroup_diffs <- Waldtest(PC_model, splitcr = subgroups)` to generate the DIF results specific to item-category thresholds, and then proceed as in Part two.

However, if you want to make comparisons at the overall item level, you’ll need to get item difficulty estimates and standard errors for the overall item. You can do this using the following code.

```{r}
### DIF analysis

# Create subgroup classifications:
subgroups <- rep(c(1,2), nrow(pcmdat2)/2)

# Calculate subgroup-specific item difficulty values (#select/highlight code starting here):

#- First, get overall item difficulties specific to each subgroup:

group1_item.diffs.overall <- NULL
group2_item.diffs.overall <- NULL

responses <- pcmdat2 # update this if needed with the responses object
model.results <- PC_model # update this if needed with the model results object

responses.g <- cbind.data.frame(subgroups, responses)
responses.g1 <- subset(responses.g, responses.g$subgroups == 1)
responses.g2 <- subset(responses.g, responses.g$subgroups == 2)

## Compare thresholds between groups:
subgroup_diffs <- Waldtest(PC_model, splitcr = subgroups)


for(item.number in 1:ncol(responses)){
  
  n.thresholds.g1 <-  length(table(responses.g1[, item.number+1]))-1
  
  group1_item.diffs.overall[item.number] <- mean(subgroup_diffs$betapar1[((item.number*(n.thresholds.g1))-(n.thresholds.g1-1)): 
                                                                           (item.number*(n.thresholds.g1))])*-1
  
  n.thresholds.g2 <-  length(table(responses.g2[, item.number+1]))-1
  
  group2_item.diffs.overall[item.number] <- mean(subgroup_diffs$betapar2[((item.number*(n.thresholds.g2))-(n.thresholds.g2-1)): 
                                                                           (item.number*(n.thresholds.g2))])*-1
}

group1_item.diffs.overall
group2_item.diffs.overall

## Get overall item SE values:

#- First, get overall SEs specific to each subgroup:

group1_item.se.overall <- NULL
group2_item.se.overall <- NULL

responses <- pcmdat2 # update this if needed with the responses object
model.results <- PC_model # update this if needed with the model results object

responses.g <- cbind.data.frame(subgroups, responses)
responses.g1 <- subset(responses.g, responses.g$subgroups == 1)
responses.g2 <- subset(responses.g, responses.g$subgroups == 2)

subgroup_diffs <- Waldtest(PC_model, splitcr = subgroups)


for(item.number in 1:ncol(responses)){
  
  n.thresholds.g1 <-  length(table(responses.g1[, item.number+1]))-1
  
  group1_item.se.overall[item.number] <- mean(subgroup_diffs$se.beta1[((item.number*(n.thresholds.g1))-(n.thresholds.g1-1)): 
                                                                        (item.number*(n.thresholds.g1))])
  
  n.thresholds.g2 <-  length(table(responses.g2[, item.number+1]))-1
  
  group2_item.se.overall[item.number] <- mean(subgroup_diffs$se.beta2[((item.number*(n.thresholds.g2))-(n.thresholds.g2-1)): 
                                                                        (item.number*(n.thresholds.g2))])
}

group1_item.se.overall
group2_item.se.overall

```

Then, we will use our own code to calculate test statistics for the differences in overall item difficulties:
  
  ```{r}
# Calculate test statistics for item comparisons:
z <- (group1_item.diffs.overall - group2_item.diffs.overall)/
  sqrt(group1_item.se.overall^2 + group2_item.se.overall^2)

# view test statistics in the console:
z
```

### Make a scatterplot

```{r}
# Plot the test statistics:
min.y <- ifelse(ceiling(min(z)) > -3, -3, 
                ceiling(min(z)))

max.y <- ifelse(ceiling(max(z)) < 3, 3, 
                ceiling(max(z)))

plot(z, ylim = c(min.y, max.y),
     ylab = "Z", xlab = "Item", main = "Test Statistics for Item Comparisons \nbetween Subgroup 1 and Subgroup 2",
     axes=FALSE)
axis(1, at = c(1, 2, 3, 4), labels = c(1, 2, 3, 4))
axis(2)
abline(h=2, col = "red", lty = 2)
abline(h=-2, col = "red", lty = 2)

legend("topright", c("Z Statistic", "Boundaries for Significant Difference"),
       pch = c(1, NA), lty = c(NA, 2), col = c("black", "red"), cex = .7)

```

Export item comparison results to a .csv file

```{r}
# Export item comparison results:

comparison.results <- cbind.data.frame(c(1:length(group1_item.diffs.overall)),
                                       group1_item.diffs.overall, group1_item.se.overall,
                                       group2_item.diffs.overall, group2_item.se.overall,
                                       z)

names(comparison.results) <- c("Item",
                               "Subgroup_1_Difficulty", "Subgroup_1_SE",
                               "Subgroup_2_Difficulty", "Subgroup_2_SE",
                               "Z")

comparison.results <- round(comparison.results, digits = 2)
write.csv(comparison.results, file = "PCM_comparison_results.csv", row.names = FALSE)

```

### Create scatterplot of item measures

Create scatterplot of item measures with 95% confidence bands

```{r}
### DIF Scatterplot:

## First, calculate values for constructing the confidence bands:

mean.1.2 <- ((group1_item.diffs.overall - mean(group1_item.diffs.overall))/2*sd(group1_item.diffs.overall) +
               (group2_item.diffs.overall - mean(group2_item.diffs.overall))/2*sd(group2_item.diffs.overall))

joint.se <- sqrt((group1_item.se.overall^2/sd(group1_item.diffs.overall)) +
                   (group2_item.se.overall^2/sd(group2_item.diffs.overall)))


upper.group.1 <- mean(group1_item.diffs.overall) + ((mean.1.2 - joint.se )*sd(group1_item.diffs.overall))
upper.group.2 <- mean(group2_item.diffs.overall) + ((mean.1.2 + joint.se )*sd(group2_item.diffs.overall))

lower.group.1 <- mean(group1_item.diffs.overall) + ((mean.1.2 + joint.se )*sd(group1_item.diffs.overall))
lower.group.2 <- mean(group2_item.diffs.overall) + ((mean.1.2 - joint.se )*sd(group1_item.diffs.overall))


upper <- cbind.data.frame(upper.group.1, upper.group.2)
upper <- upper[order(upper$upper.group.1, decreasing = FALSE),]


lower <- cbind.data.frame(lower.group.1, lower.group.2)
lower <- lower[order(lower$lower.group.1, decreasing = FALSE),]

## make the scatterplot:

plot(group1_item.diffs.overall, group2_item.diffs.overall, xlim = c(-3, 3), ylim = c(-3, 3),
     xlab = "Group 1", ylab = "Group 2", main = "Group 1 Measures \n plotted against \n Group 2 Measures")
abline(a = 0, b = 1, col = "purple")

par(new = T)

lines(upper$upper.group.1, upper$upper.group.2, lty = 2, col = "red")

lines(lower$lower.group.1, lower$lower.group.2, lty = 2, col = "red")

legend("bottomright", c("Item Location", "Identity Line", "95% Confidence Band"),
       pch = c(1, NA, NA), lty = c(NA, 1, 2), col = c("black", "purple", "red"))

```

### Create bar plot of item differences

```{r}
### Bar plot of item differences:

# First, calculate difference in difficulty between subgroups
item_dif <- group1_item.diffs.overall - group2_item.diffs.overall

# Code to use different colors to highlight items with differences >= .5 logits:
colors <- NULL

for (item.number in 1:ncol(responses)){
  colors[item.number] <- ifelse(abs(item_dif[item.number]) > .5, "dark blue", "light green")
}

# Bar plot code:
item_dif <- as.vector(item_dif)

x <- barplot(item_dif, horiz = TRUE, xlim = c(-2, 2), 
             col = colors,
             #=ylim = c(1,4), 
             xlab = "Logit Difference")

# code to add labels to the plot:

dif_labs <- NULL

for (i in 1:length(group1_item.diffs.overall)) {
  dif_labs[i] <- ifelse(item_dif[i] < 0, item_dif[i] - .2,
                        item_dif[i] + .2)
}

text(dif_labs, x, labels = c(1:length(group1_item.diffs.overall)),
     xlim = c(-1.5, 1.5), cex = .8)

# add vertical lines to highlight .5 logit differences:
abline(v = .5, lty = 3)
abline(v = -.5, lty = 3)

# add additional text to help with interpretation:

text(-1, 4.5, "Easier to Endorse for Group 1", cex = .8)
text(1, 4.5, "Easier to Endorse for Group 2", cex = .8)

legend("bottomright", c("Diff >= .5 logits", "Diff < .5 logits"),
       pch = 15, col = c("dark blue", "light green"), cex = .7 )

```

## Example APA-format results section for basic DIF analysis

In this analysis, we examined the degree to which there was evidence of uniform differential item functioning (DIF) between two subgroups of participants (group 1 and group 2) in the “pcmdat2” example dataset from the using the Extended Rasch Models (“eRm”) package (Mair, Hatzinger, & Maier, 2020) for R. The data included 300 participants’ responses to four rating scale items made up of three ordered categories (*X* = 0, 1, or 2). We used the eRm package to conduct all of the analyses.

As a first step in the analysis, we examined item difficulty estimates and standard errors specific to each subgroup using the Partial Credit model (Masters, 1982). We estimated item difficulty for the two subgroups using a combined analysis of both subgroups and then estimating the group-specific item difficulties using the eRm package. Group-specific item difficulties and standard errors are shown in Table 1.

To examine the differences in item difficulty between subgroups, we calculated standardized differences following Wright and Masters (1982) as follows: $$z=\left(d_{1}-d_{2}\right) / \sqrt{se_{1}^{2}+se_{2}^{2}}$$
  
  where $z$ is the standardized difference, d1 is the item difficulty specific to Subgroup 1, d2 is the item difficulty specific to Subgroup 2, $se_{1}^{2}$ is the standard error of the item difficulty specific to Subgroup 1, and $se_{2}^{2}$ is the standard error of the item difficulty specific to Subgroup 2. Using the formulation of the z statistic, higher values of z indicate higher item locations (more-difficult to endorse) for Subgroup 1 compared to Subgroup 2.   

Figure 1 shows a plot of the z-statistics for the four survey items; these values are also presented numerically in Table 1. In the figure, the x-axis shows the item identification numbers, and the y-axis shows the value of the z-statistic. Boundaries at +2 and -2 are indicated using dashed horizontal lines to highlight statistically significant differences in item difficulty between subgroups. Examination of these results indicates that the items were not significantly different in difficulty between the two subgroups. In addition, there were both positive and negative z statistics, indicating that although the differences in item difficulty were not significant, there were some items that were easier to endorse for Subgroup 1 and others that were easier to endorse for Subgroup 2.

![Figure 1: Plot of Standardized Differences for Items between Subgroups](DIF_figure1.png)  

To further explore the differences in item difficulty between the two subgroups, Figure 2 shows a scatterplot of the item locations between the two subgroups. In the plot, the item difficulty for Subgroup 1 is shown on the x-axis, and the item difficulty for Subgroup 2 is shown on the y-axis. Individual items are indicated using open circle plotting symbols. A solid identity line is included to highlight deviations from invariant item difficulties between the two groups: Points that fall below this line indicate that items were easier to endorse (lower item measures) for Subgroup 2, and points that fall above the line indicate that items were easier to endorse (lower item measures) for Subgroup 1. Dashed lines are also included to indicate a 95% confidence interval for the difference between the item measures, following Luppescu (1995).

![Figure 2: Scatterplot of Subgroup-Specific Item Difficulties](DIF_figure2.png)

Finally, Figure 3 is a bar plot that illustrates the direction and magnitude of the differences in item difficulty between subgroups. In the plot, each bar represents the difference in difficulty between subgroups for an individual item, ordered by the item sequence in the survey. Bars that point to the left of the plot indicate that the item was easier to endorse for Subgroup 1, and bars that point to the right of the plot indicate that the item was easier to endorse for Subgroup 2. Dashed vertical lines are plotted that show values of +0.5 and -0.5 logits as an indicator of substantial differences in item difficulty between subgroups. 

![Figure 3: Bar Plot of Differences In Item Difficulty Between Subgroups](DIF_figure3.png)  

## References

Luppescu, S. (1995). Comparing measures: Scatterplots. Rasch Measurement Transactions, 9(1), 410.

Masters, G. N. (1982). A Rasch model for partial credit scoring. Psychometrika, 47(2), 149–174. https://doi.org/10.1007/BF02296272

Masters, G. N., & Wright, B. D. (1997). The partial credit model. In W. J. van der Linden & R. K. Hambleton (Eds.), Handbook of modern item response theory (pp. 101–121). Springer.

Mair, P., Hatzinger, R., & Maier M. J. (2020). eRm: Extended Rasch Modeling. 1.0-1. https://cran.r-project.org/package=eRm

Wright, B. D., & Masters, G. N. (1982). Rating Scale Analysis: Rasch Measurement. MESA Press.
